{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:\n",
    "langchain for the ai in app integration and openai specific integration\n",
    "chromadb is the vector database for storing and querying data\n",
    "pypdf for parsing and reading pdfs in python\n",
    "pandas for data manipulation and analysis\n",
    "streamlit for the app UI\n",
    "dotenv for managing environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet langchain-community langchain-openai chromadb\n",
    "!pip3 install --upgrade --quiet pypdf pandas streamlit python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Api key is in env file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env-01/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Import Langchain modules for alot of things\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Other modules and packages that are needed\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st  \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # reading all vaiables from .env file (api key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAPI_API_KEY = os.environ.get('OPENAI_API_KEY') # getting api key from .env file and bringing it to our notebook as a var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Active', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 12, 'total_tokens': 13, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-353728f3-5b0f-478f-8719-f06ccddd639e-0', usage_metadata={'input_tokens': 12, 'output_tokens': 1, 'total_tokens': 13, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from langchain_openai we get our llm and specify the model (4o is cheap and fast)\n",
    "    api key is optional here it will know our api key as we set the env var\"\"\"\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAPI_API_KEY)\n",
    "llm.invoke(\"if active respond with active\") # calling the llm for a prompt this is just like typing a message into chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSING THE PDF FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pdf_pages contains a list of document objects, each document object representing a page of the pdf\\n    the metadata contains the source of the document and the page number etc etc\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(\"./test data/testpaper.pdf\") # loading the pdf file from our project directory\n",
    "pdf_pages = pdf_loader.load() # loading the pages of the pdf\n",
    "pdf_pages # printing all the pages of the pdf\n",
    "\n",
    "\"\"\" pdf_pages contains a list of document objects, each document object representing a page of the pdf\n",
    "    the metadata contains the source of the document and the page number etc etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem, right now the pdfpages contains the whole pdf as you might have through there is no way we will put in a multi page reserach paper into open ai's llm model, firstly there a token limit, secondly and more importantly we need to specify parts in the document to get good results i.e the llm dose not need every word in the pdf, hence we only want to feed the most relevent part into the llm promt. passing too much info/ irelevent info to the llm gives bad results.\n",
    "- Solution, split the pdf into smaller chunks like paragaphs/ sentences. as we slipt he document into smaller chunks each chunk will be more relevent and contain less data making our resulting prompt more accurate and more likely to get good results from the llm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './test data/testpaper.pdf', 'page': 0}, page_content='ChatDyn: Language-Driven Multi-Actor Dynamics Generation\\nin Street Scenes\\nYuxi Wei1 Jingbo Wang2† Yuwen Du1 Dingju Wang1\\nLiang Pan2 Chenxin Xu1 Yao Feng3,4 Bo Dai2 Siheng Chen1†\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory\\n3 Max Planck Institute for Intelligent Systems 4 ETH Z¨urich\\n{wyx3590236732, wangdingju, xcxwakaka, sihengc}@sjtu.edu.cn, duyuwen@tju.edu.cn\\n{wangjingbo, panliang, daibo}@pjlab.org.cn, yao.feng@tuebingen.mpg.de\\nvfishc.github.io/chatdyn\\nAbstract\\nGenerating realistic and interactive dynamics of traffic\\nparticipants according to specific instruction is critical for\\nstreet scene simulation. However, there is currently a lack\\nof a comprehensive method that generates realistic dynam-\\nics of different types of participants including vehicles and\\npedestrians, with different kinds of interactions between\\nthem. In this paper, we introduce ChatDyn, the first system\\ncapable of generating interactive, controllable and realistic\\nparticipant dynamics in street scenes based on language in-\\nstructions. To achieve precise control through complex lan-\\nguage, ChatDyn employs a multi-LLM-agent role-playing\\napproach, which utilizes natural language inputs to plan\\nthe trajectories and behaviors for different traffic partici-\\npants. To generate realistic fine-grained dynamics based\\non the planning, ChatDyn designs two novel executors:\\nthe PedExecutor, a unified multi-task executor that gener-\\nates realistic pedestrian dynamics under different task plan-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 0}, page_content='on the planning, ChatDyn designs two novel executors:\\nthe PedExecutor, a unified multi-task executor that gener-\\nates realistic pedestrian dynamics under different task plan-\\nnings; and the VehExecutor, a physical transition-based\\npolicy that generates physically plausible vehicle dynam-\\nics. Extensive experiments show that ChatDyn can gen-\\nerate realistic driving scene dynamics with multiple vehi-\\ncles and pedestrians, and significantly outperforms previ-\\nous methods on subtasks. Code and model will be available\\nat https://vfishc.github.io/chatdyn.\\n1. Introduction\\nAutonomous driving simulation plays a critical role in the\\ntraining and validation of driving systems, which attracts\\nsignificant attention in recent years. To create a realistic\\nsimulation system, it must replicate real world scenarios, in-\\n†Corresponding authors.\\nI want to \\nadd a person \\nis taking taxi and ….\\nUser with scenario information\\n“The scene \\nshould be…”\\n… …\\nChatDyn\\nTwo stage:\\nplanning and generation\\nScene dynamics\\nPrecise control\\nDiverse interaction\\nRealistic output\\nHigh-level planning\\nLow-level generation\\nNo fine-grained interaction\\nStruggle in precise control\\nInsufficient interaction\\nPrevious works\\n Ours\\nFigure 1. ChatDyn achieves interactive and realistic language-\\ndriven multi-actor dynamics generation in street scenes.\\ncluding the dynamics of various traffic participants such as\\npedestrians and vehicles [21, 23, 24], as well as their inter-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 0}, page_content='driven multi-actor dynamics generation in street scenes.\\ncluding the dynamics of various traffic participants such as\\npedestrians and vehicles [21, 23, 24], as well as their inter-\\nactions. These dynamics are essential for constructing real-\\nistic scene events, maintaining the temporal consistency of\\nsimulations [8, 14, 60, 73], and enabling accurate decision\\nmaking [2, 48, 66] in autonomous systems. However, gen-\\nerating such dynamics is challenging, as it involves mod-\\neling the behavior of vehicles and pedestrians individually\\nwhile also capturing their intricate interactions within street\\nscenes. This requires high-level planning to coordinate tra-\\njectory generation for both types of participants, taking the\\nstreet scenes into account, and precise low-level control to\\nensure that their interactions appear realistic.\\nExisting methods often focus on one of these aspects\\nwhile neglecting the other. High-level planning approaches,\\nsuch as LCTGen [63] and CTG++[82], use language-based\\ninputs to generate scene-level plans. However, they strug-\\ngle to control detailed participant behaviors and often lack\\ndiversity in participants, such as pedestrians, as well as the\\nvaried interactions between different participants. On the\\nother hand, low-level generation methods, such as Pacer[56]\\nand Pacer+ [69], excel at producing fine-grained pedestrian\\ndynamics based on specific control signals. Yet, they fail\\n1\\narXiv:2412.08685v1  [cs.CV]  11 Dec 2024'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 1}, page_content='to account for the nuanced interactive behaviors that arise\\nbetween pedestrians and other participants.\\nIn this work, we introduce ChatDyn, the first system\\nachieving interactive and realistic language-driven multi-\\nactor dynamics generation in street scenes, as shown in\\nFig. 1. The key insight behind ChatDyn is the need to\\ncombine both high-level planning and low-level control to\\ncreate a realistic simulation system. Large language mod-\\nels (LLMs) excel at high-level planning, while physics-\\nbased methods are well-suited for fine-grained, low-level\\ncontrol. Building on this observation, ChatDyn combines\\na multi-LLM agent for high-level planning with physics-\\nbased executors for low-level generation. By integrating\\nhigh- and low-level components, ChatDyn allows users to\\ninput language instructions to specify their requirements. It\\nthen generates realistic dynamics for diverse traffic partici-\\npants, including pedestrians and vehicles, within the speci-\\nfied scene, while accommodating complex and varied inter-\\nactions among participants.\\nSpecifically, ChatDyn employs multi-LLM-agent role-\\nplaying for high-level planning to understand complex and\\nabstract user instructions. The key idea is to treat each traf-\\nfic participant as an agent, which can extract specific re-\\nquirements and semantics from the language instructions,\\ninteract according to the requirement of scenes, and gener-\\nate planned trajectories and behaviors with interactive char-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 1}, page_content='quirements and semantics from the language instructions,\\ninteract according to the requirement of scenes, and gener-\\nate planned trajectories and behaviors with interactive char-\\nacteristics. The multi-LLM-agent role-playing approach\\nshows two notable advantages: first, LLM has powerful\\nlanguage understanding capabilities, enabling them to accu-\\nrately capture the complex and abstract demands in user in-\\nstructions, thus producing planning results that closely align\\nwith them; second, the interaction between agents allows\\nfor the incorporation of interactive information during the\\nplanning process, ensuring that the planning results, at the\\nhigh-level stage, exhibit interactive characteristics.\\nDue to the different characteristics of the dynamics of\\npedestrians and vehicles, we design separate executors for\\ntheir low-level generation. To generate realistic and interac-\\ntive fine-grained low-level pedestrian dynamics, we propose\\nPedExecutor, a unified method accomplishes multiple high-\\nlevel planned tasks, including fine-grained physical inter-\\nactions, and returns human-like dynamics. PedExecutor is\\na physics-based control policy that fulfills multiple control\\nrequirements and enables interactive tasks for pedestrians\\nunder realistic physical feedback. A task masking mecha-\\nnism and related training strategy make it handle different\\nrequirements with a single unified policy. PedExecutor also\\nincorporates hierarchical control and body-masked adver-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 1}, page_content='nism and related training strategy make it handle different\\nrequirements with a single unified policy. PedExecutor also\\nincorporates hierarchical control and body-masked adver-\\nsarial motion prior, which introduce priors to both the ac-\\ntion space and reward space, allowing for the generation of\\nrealistic, natural dynamics while executing the planning.\\nTo obtain physically feasible low-level vehicle dynam-\\nics, we propose VehExecutor, a control policy based on\\nvehicle physical transition process. VehExecutor gener-\\nates vehicle actions from high-level trajectory planning, and\\nphysical transition process returns the next vehicle state\\nwhich can be accumulated as final dynamics output. Ve-\\nhExecutor ensures the physical feasibility of output dynam-\\nics and eliminating unrealistic factors such as tail swings\\nand unwanted translations. Additionally, VehExecutor in-\\ncorporates history-aware state and action design, simulating\\nthe real-world decision-making process of vehicles, which\\nimproves the accuracy and consistency of control.\\nBased on the above designs, ChatDyn satisfied the three\\nrequired characteristics: for interactivity, multi-LLM-agent\\nrole-playing enables interaction planning at the high-level,\\nwhile the executors execute the interaction-aware plans to\\ngenerate low-level interactive behaviors; for controllability,\\nLLM-agents precisely follow the user’s natural language in-\\nputs, and the executors precisely execute the planning for'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 1}, page_content='generate low-level interactive behaviors; for controllability,\\nLLM-agents precisely follow the user’s natural language in-\\nputs, and the executors precisely execute the planning for\\naccurate control; for realism, PedExecutor employs hierar-\\nchical control and integrates body-masked adversarial mo-\\ntion priors, producing dynamics that align with human intu-\\nitions, while VeExecutor highly simulates the physical feed-\\nback and kinematic characteristics of real vehicle.\\nOur contributions can be summarized as: (1) We de-\\nsign the first language-driven dynamics generation system\\nfor multi-actor in street scenes, utilizing multi-LLM-agent\\nrole-playing for high-level planning. (2) We propose a uni-\\nfied pedestrian executor that, under hierarchical control, ex-\\necutes multiple planning tasks and generates realistic low-\\nlevel pedestrian dynamics based on planning. (3) We pro-\\npose a vehicle executor that derives realistic control pro-\\ncesses and low-level vehicle dynamics from planning tra-\\njectories. (4) We demonstrate the realistic language-driven\\noutputs of the system in experiments and validated the ef-\\nfectiveness of its each individual component.\\n2. Related Works\\nHuman dynamics generation. The generation of hu-\\nman dynamics can broadly be divided into kinematics-\\nbased and physics-based approaches. For kinematics-based\\ngeneration, transformer-based methods [3, 25, 26] and dif-\\nfusion models [11, 33, 80, 81] are used to generate cor-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 1}, page_content='based and physics-based approaches. For kinematics-based\\ngeneration, transformer-based methods [3, 25, 26] and dif-\\nfusion models [11, 33, 80, 81] are used to generate cor-\\nresponding dynamics based on language inputs. Recent\\nmethods [58, 68, 74] further introduce more control condi-\\ntions, while others [12, 85] focus on improving efficiency\\nin the generation process. However, these approaches\\ndo not account for physical constraints, often resulting\\nin physically implausible results, particularly for interac-\\ntions. For physics-based generation, existing work like\\n[15, 44, 45, 51, 52, 71, 79] achieve predefined tasks with\\nplausible dynamics. [4, 32, 65, 75, 76] extend the human\\ncharacter dynamics content with different designs. [56, 69]\\nfocus on the dynamics of street scene pedestrians. How-\\never, these methods do not involve the interaction behaviors\\n2'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 2}, page_content='between multi-pedestrians. Our PedExecutor considers the\\ninteraction behaviors, and is trained as a unified policy for\\nmultiple scenarios also including following, imitation.\\nVehicle dynamics generation. In industry, vehicle dy-\\nnamics are usually generated by software tools [9, 19, 31,\\n54]. Some research works [6, 17, 55, 62] focus on un-\\nconditioned generation to simplify the generation process.\\nRecent works on vehicle dynamics generation under differ-\\nent conditions have introduced various approaches. Traffic-\\nSim [61] uses predictive models to generate vehicle dynam-\\nics in a scene, while CTG [83] introduces diffusion models\\ninto the generation process. CTG++ [82] and LCTGen [63]\\nachieve scene-vehicle dynamics generation controlled by\\nlanguage, and SceneControl [43] adds multiple controllable\\nparameters to the generation process. RealGen [13] uses\\na retrieval-based approach to improve the realism of gen-\\nerated dynamics. However, these works generally do not\\ndirectly consider physical constraints and kinematic con-\\nstraints, and lack sufficient interaction. And these language-\\ncontrolled methods cannot achieve precise control over dif-\\nferent vehicles. Our LLM-agents planning utilizes the lan-\\nguage understanding capability of LLMs to achieve precise\\ncontrol of participants considering interactive information,\\nand VehExecutor generates the final physical plausible ve-\\nhicle dynamics.\\nLarge language models and agents. Recently, numer-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 2}, page_content='control of participants considering interactive information,\\nand VehExecutor generates the final physical plausible ve-\\nhicle dynamics.\\nLarge language models and agents. Recently, numer-\\nous large language models [1, 5, 22, 39, 67, 78] are pro-\\nposed and released. Many works have leveraged these mod-\\nels by finetuning them[18, 30, 40, 53] or integrating them\\nwith relevant tools to build LLM-based agents [41, 72] ca-\\npable of performing targeted tasks or specific functions.\\nThese agents have been applied to a wide range of down-\\nstream tasks, including code development [28, 29], web\\nbrowsing [34, 84], medical Q&A [36, 37], social behav-\\nior simulation [35, 50], and issue handing [59, 64]. In this\\npaper, we explore the application of agents in traffic simu-\\nlation, utilizing them to interact and perform trajectory and\\nbehavior planning within traffic scenarios.\\n3. Method\\n3.1. System Overview\\nChatDyn interprets and analyzes user language instruc-\\ntions, then produces scene dynamics that align with them.\\nChatDyn employs a two-stage process: high-level planning\\nwhich plans trajectory and behavior under complex and ab-\\nstract command; low-level generation for fine-grained, re-\\nalistic dynamics generation. Since user instructions may\\ncontain many specific details that require precise control\\nand abstract semantics that need to be understood, Chat-\\nDyn leverages multi-LLM-agent role-playing, treating each\\ntraffic participant as an LLM-agent. This approach capital-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 2}, page_content='and abstract semantics that need to be understood, Chat-\\nDyn leverages multi-LLM-agent role-playing, treating each\\ntraffic participant as an LLM-agent. This approach capital-\\nizes on the LLM’s ability to comprehend semantic informa-\\n“My position is …, \\nyou can ….”\\n“My start point is …, \\nyou can….”\\n“My keypoints …., I will \\nwave my hand”\\n“My keypoints should \\nbe…. to pull over”\\nPlanned trajectory\\nOracle \\nAgent\\nUser\\nI want to add a man is \\ntaking taxi, a pedestrian \\nwalks with arms around another’s shoulder.\\n“There are … actors, their attributes are… \\nand the interaction groups are….”\\nLow-level generation\\nPlanned trajectory\\nBehavior\\n(e.g. wave hand, \\npush)\\nPedestrian Executor\\nPlanned trajectory\\nVehicle Executor\\nHigh-level planning\\nPedestrain dynamics Vehicle dynamics\\n“My trajectory is …, \\nyou can ….”\\n“I will …”\\n“My keypoints should be…”\\n“My keypoints …, and I \\nshould …”\\nInteraction\\nInteraction\\nFigure 2. System overview. ChatDyn adopts multi-LLM-agent\\nrole-playing for precise high-level planning. Two specialized ex-\\necutors are designed for realistic low-level generation.\\ntion and its extensive commonsense priors, using specific\\ntools and interaction process to complete high-level trajec-\\ntory and behavior planning. Each traffic participant’s cor-\\nresponding agent is also equipped with an executor as one\\nof the tools. After the high-level planning is completed, the\\nexecutor uses the planning results to execute the low-level'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 2}, page_content='responding agent is also equipped with an executor as one\\nof the tools. After the high-level planning is completed, the\\nexecutor uses the planning results to execute the low-level\\ngeneration process. The executors generate fine-grained,\\nrealistic, and physically feasible dynamics based on high-\\nlevel planning. See Fig. 2 for illustration.\\n3.2. High-Level Planning\\nHigh-level planning is completed by LLM-agents and\\ninteraction among them. Each LLM-agent consists of an\\nLLM component and the relevant tools, and the agents are\\ndivided into two types: oracle agents and actor agents.\\nThe oracle agent is responsible for directly receiving user\\ninstruction and understanding, dispatching them. Oracle\\nagent breaks the instructions down into specific informa-\\ntion for each participant, identifies interaction relationships,\\nforms interaction groups, and generates the planning order\\nschedule for the following process. Each actor agent cor-\\nresponds to a specific traffic participant, initialized with the\\ninformation provided by the oracle agent. The actor agents\\nthen interact with others based on the interaction groups to\\nget information from others, and finally plan trajectory and\\nbehavior using their tools. The following parts will provide\\na detailed description of these two types of agents.\\n3.2.1. Oracle Agent\\nOracle agent breaks down complex and lengthy user in-\\nstructions into specific actions for each participant, ensur-\\n3'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 3}, page_content='ing an accurate understanding of the user’s instructions and\\ngenerating the subsequent schedule. For its LLM compo-\\nnent, we explain its responsibilities, outline our require-\\nments and give some few-shot examples. The LLM com-\\nponent of the oracle agent outputs the initialization informa-\\ntion for each participants, interaction groups, and the sched-\\nule in natural language. To facilitate subsequent execution,\\nwe equip it with a structured output tool that converts lan-\\nguage output into a predefined data structure for execution.\\nOracle agent provides the system with the ability to handle\\ncomplex, mixed, and abstract instructions, and streamlines\\noperations for clarity and fine granularity. Detailed instruc-\\ntion prompt can be found in the appendix.\\n3.2.2. Actor Agent\\nEach actor agent corresponds to a specific participant in\\nthe scene and is initialized with information provided by the\\noracle agent, including the agent type and description about\\nits trajectory and behavior. The actor agents interact based\\non the oracle agent’s schedule to collect interactive informa-\\ntion and completes the final high-level planning. The plan-\\nning process of the actor agent incorporates map informa-\\ntion, with the scene map represented as a graphG = (N, E),\\nwhere N represents different lane sections. Each node in\\nN contains the point set for the lane section and other ex-\\ntracted information such as its orientation and driving type'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 3}, page_content='where N represents different lane sections. Each node in\\nN contains the point set for the lane section and other ex-\\ntracted information such as its orientation and driving type\\n(e.g., straight, left/right turn, lane/boundary). The edges E\\nrepresent the relationships between the lane sections, such\\nas adjacency or connectivity. The specific planning process\\nuses a keypoints-based approach, integrating actor interac-\\ntions to complete the planning.\\nKeypoints-based trajectory planning. Intuitively, a\\ntrajectory is typically determined by a series of keypoints,\\nwhich are then interpolated to form the complete trajectory.\\nSpecifically, the actor agent first obtains keypoints and then\\nuses interpolation to get the planned trajectory. The LLM\\ncomponent of the actor agent infers the number of key-\\npoints needed based on its initialization information, and\\nthen determines the generation process for each keypoint.\\nThere are generally two sources for keypoints: one is di-\\nrectly obtained from map and previous keypoints, where\\nthe actor agent retrieves keypoints from the mapG based on\\nthe keypoint’s attributes and relationship with the previous\\nkeypoint. The other source is through interaction, where a\\nkeypoint depends on information from others. This kind of\\nkeypoints are determined by actor interaction.\\nMulti-actor interaction. Interactions allow agents to\\nacquire information from other agents to generate certain\\nkeypoints, thereby fulfilling the interaction requirements'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 3}, page_content='Multi-actor interaction. Interactions allow agents to\\nacquire information from other agents to generate certain\\nkeypoints, thereby fulfilling the interaction requirements\\nduring the high-level planning process. The groups that\\nneed to interact have already been specified by the oracle\\nagent. During the interaction, the LLM determines which\\npieces of information are needed from the interacting agents\\nfor a specific keypoint, such as the start point, endpoint, or\\nthe entire trajectory. Each agent then shares the required\\ninformation, and this exchanged information is provided to\\nthe corresponding tools that generate keypoints based on the\\nagent’s own needs. Actor interaction enables interactive tra-\\njectory generation, which is common in the real scenarios.\\nAfter obtaining all desired keypoints, interpolation is ap-\\nplied to generate a complete trajectory, fulfilling the trajec-\\ntory planning stage. Here, we employ B ´ezier curves for in-\\nterpolation. Note that: (i) the interpolated trajectory at this\\nstage lacks physical constraints and serves only as a pre-\\nliminary plan to be executed by the subsequent executor;\\n(ii) for static actors, two keypoints are required to deter-\\nmine orientation, as their facing direction must be specified;\\n(iii) when the actor type is pedestrian, besides trajectory,\\nadditional behavioral instructions may be inferred by LLM\\ncomponent based on the initialization information. These\\nbehaviors could stem from direct command-based controls'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 3}, page_content='additional behavioral instructions may be inferred by LLM\\ncomponent based on the initialization information. These\\nbehaviors could stem from direct command-based controls\\nor indirectly from semantic cues, with the final instructions\\nprovided in text form for the executor to execute.\\n3.3. Low-Level Generation for Pedestrian\\nThe pedestrian executor (PedExecutor), as shown in Fig.\\n3, generates low-level pedestrian dynamics based on the tra-\\njectories and behaviors planned from high-level planning.\\nPedestrian behaviors can be subdivided into single-agent\\nbehavior directly specified by language and interactive be-\\nhaviors that occur between multiple agents. Thus the chal-\\nlenge lies in simultaneously handling trajectory following,\\nsingle-agent motion specification, and multi-agent interac-\\ntions, while maintaining human-like quality. To achieve\\nthese, PedExecutor utilizes multi-task unified training to ex-\\necute the trajectory, single-agent behavior, and multi-agent\\ninteractions as planned by the LLM with a single policy.\\nFor human-like quality, the action space incorporates hier-\\narchical control to provide priors, while the reward function\\nuses body masked Adversarial Motion Priors (AMP) [51] to\\nencourage human-like control. Ultimately PedExecutor re-\\nturns realistic dynamics that follow planned trajectory and\\ncomplete desired behaviors.\\nControl process is defined by Markov decision process\\nMp = {Sp, Ap, T p, Rp, γp}, where the elements repre-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 3}, page_content='turns realistic dynamics that follow planned trajectory and\\ncomplete desired behaviors.\\nControl process is defined by Markov decision process\\nMp = {Sp, Ap, T p, Rp, γp}, where the elements repre-\\nsent states, actions, transition process, reward, and the dis-\\ncount factor. Goal-conditioned reinforcement learning (RL)\\nis adopted for training. The transition process is imple-\\nmented by the physics engine. Others are detailed below.\\nStates and multi-task unified training. Our control\\npolicy is designed to handle multiple distinct tasks, requir-\\ning specialized processing and training to achieve a uni-\\nfied policy capable of addressing different tasks. The tasks\\nare divided into three main categories based on require-\\nments: trajectory following, single-agent behavior speci-\\nfication, and multi-agent interaction. For single-agent be-\\nhavior, the LLM generates corresponding text descriptions,\\n4'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 4}, page_content='Trajectory\\nBody Motion\\nInteraction \\ntarget\\nTrajectory \\nstate\\nMotion \\nstate\\nTarget \\nstate\\nC\\nTask-related\\nstate\\nPolicy Network\\nLatent space \\nAction\\nDecoder Action \\nPhysics Simulation\\nInference\\nTraining\\nMotion references\\nBody mask\\nDiscrimination\\nNetwork\\nBody state Task reward\\nDiscrimination \\nreward\\nLearning :PPO\\nOn-demand\\ntask mask\\nRollout \\nfor output\\nMulti-task unified training\\nHierarchical control\\nAMP with body mask\\n: Plus : Multiply C : Concatenate\\nFigure 3. Pedestrian executor (PedExecutor) framework. With multi-task unified training, PedExecutor achieves unified control over\\nvarious task, including following, imitation and interaction. It generates realistic pedestrian dynamics by effectively executing tasks\\nderived from planning. Hierarchical control and AMP with body mask provide prior to action space and reward space, improving the\\nrealism of dynamics output.\\nwhich are then processed by a Text2Motion model (e.g.,\\nMoMask [26]) to produce an upper-body motion for the pol-\\nicy to imitate thereby achieve single-agent behavior speci-\\nfication. Multi-agent interaction tasks are trained according\\nto predefined types, with the LLM’s planning classifying\\ninteraction behaviors into these types, enabling the execu-\\ntor to execute specific interactions as specified.\\nTask-related states consist of trajectory slices to be fol-\\nlowed Sp\\ntraj, motion to be imitated Sp\\nmo, and states of\\ninteracting targets Sp\\ntar. Sp\\ntraj includes K future steps'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 4}, page_content='Task-related states consist of trajectory slices to be fol-\\nlowed Sp\\ntraj, motion to be imitated Sp\\nmo, and states of\\ninteracting targets Sp\\ntar. Sp\\ntraj includes K future steps\\nof planned trajectory from the current timestep, while\\nSp\\nmo = concat(ˆjpos, ˆjrot, ˆjvel, ˆjω) comprises joint position\\nˆjpos, joint rotation ˆjrot, joint velocity ˆjvel, and rotation ve-\\nlocity ˆjω from the motion, with optional joint masking to\\nretain only relevant parts, such as upper body. Sp\\ntar =\\nconcat(rpos, rrot, rvel, rω, rbbox, einter) includes the inter-\\nacting target’s root position rpos, root rotation rrot, root ve-\\nlocity rvel, root angular velocityrω, and the bounding box’s\\n8 vertices rbbox. Additionally, an interaction task embed-\\nding einter represented as a one-hot vector to specify the\\ninteraction type. Beyond task-related states, the final obser-\\nvations also include humanoid proprioception [69] Sp\\nprop to\\ncapture necessary internal states of the humanoid. The final\\nSp = concat(Sp\\ntraj, Sp\\nmo, Sp\\ntar, Sp\\nprop)\\nTo activate specific parts during training/testing, we in-\\ntroduce a task masking mechanism. During training, tasks\\nto be executed for each episode are sampled, and the re-\\nmaining tasks related states are masked. Only the relevant\\nenvironment for the involved tasks is prepared, and rewards\\nare calculated based on that. During inference, states are ac-\\ntivated as needed, while others are masked, enabling task-\\nspecific execution. This approach ensures the trained uni-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 4}, page_content='are calculated based on that. During inference, states are ac-\\ntivated as needed, while others are masked, enabling task-\\nspecific execution. This approach ensures the trained uni-\\nfied policy handles all of the tasks without confusion. After\\nsufficient training, its performance matches that of policies\\ntrained on individual tasks, while can also execute multiple\\nnon-conflicting tasks in a single run.\\nAction hierarchical control. Pedestrian action spaces\\nare typically modeled using a proportional-derivative (PD)\\ncontroller at each degree of freedom (DoF), but such spaces\\nlack inherent priors, often leading to locally unrealistic ac-\\ntions for completing specific tasks. To address this, we ap-\\nply a hierarchical action control space from PULSE [45].\\nThe policy network first outputs to a pretrained latent space,\\nobtaining a latent feature faction, which is then decoded by\\na pretrained decoder into control signals. The pretrained\\nlatent space, equipped with the corresponding decoder, en-\\nsures that the decoded output distribution closely matches\\nthe input data from pretraining, thereby providing the ac-\\ntion space with a real-world action prior.\\nReward design. The reward consists of two main com-\\nponents. The first is the discrimination reward Rdisc used\\nto implement AMP [51], which employs a discriminator\\nto encourage the policy to generate output that align with\\nmovement patterns observed in a dataset of human-recorded'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 4}, page_content='to implement AMP [51], which employs a discriminator\\nto encourage the policy to generate output that align with\\nmovement patterns observed in a dataset of human-recorded\\ndata clips. The second component is the task-related reward\\nRtask , designed to motivate the policy to accomplish spe-\\ncific tasks for executing high-level planning. The detailed\\ntask reward designs can be found in appendix. Similar to\\n[69], we implement early termination if excessive root dis-\\ntance error or joint distance error occurs during following\\nor imitation.\\nAMP with body mask and warm-up training. We\\nemploy Proximal Policy Optimization (PPO) [57] for over-\\nall training optimization. However, directly optimizing for\\ninteraction tasks presents certain challenges: without using\\nAMP, the lack of a discrimination reward makes it difficult\\n5'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 5}, page_content='Inference\\nTraining\\nCurrent state\\nActions\\nNext vehicle state \\nTracking reward \\n||Action||2\\nSmoothness reward \\nUpdate\\nRollout \\nfor output\\nPolicy Network\\nPhysical transition\\nFigure 4. Vehicle executor (VehExecutor) framework. VehExecu-\\ntor adopts goal-conditioned RL based on physical transition of real\\nvehicle. Combining with history-aware design, VehExecutor gen-\\nerates realistic vehicle dynamics under planned trajectory.\\nto achieve human-like dynamics results. Conversely, proper\\nreference data clip needed for specific tasks are usually\\ncomplex and not easily obtainable. Using reference data\\nclip from the following or imitation tasks can lead to mis-\\nalignment with the actual requirements for interaction, re-\\nsulting in a policy that fails to accomplish interaction tasks.\\nTo address these issues, we adopt two training tech-\\nniques: (i) Initially, we train without AMP. While this ap-\\nproach may not yield realistic dynamics, it allows the pol-\\nicy to complete interaction tasks, providing a more accurate\\ninitial domain for subsequent optimization. Once this foun-\\ndation is established, we incorporate AMP training, which\\nmaintains task completion while improving dynamics qual-\\nity. (ii) We introduce AMP body mask, where certain joints\\nare excluded during the AMP calculations. Since interac-\\ntion tasks primarily engage specific body part(e.g. arm), we\\napply the AMP mask to the relevant body parts, ensuring\\nthat the rest parts of dynamics can leverage AMP for more'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 5}, page_content='tion tasks primarily engage specific body part(e.g. arm), we\\napply the AMP mask to the relevant body parts, ensuring\\nthat the rest parts of dynamics can leverage AMP for more\\nnatural movements while allowing the specific part to com-\\nplete the tasks. By utilizing these two training techniques,\\nwe achieve a policy that satisfies both visual realism and the\\nability to perform interaction tasks effectively.\\n3.4. Low-Level Generation for Vehicles\\nThe vehicle executor (VehExecutor), as shown in Fig.\\n4, generates the final realistic and physically feasible ve-\\nhicle dynamics with control policy based on the high-\\nlevel planned trajectory, which may initially violate cer-\\ntain dynamic constraints. To involve physical constraints\\nand achieve precise control, VehExecutor utilizes physics-\\nbased transition environment, combined with history-aware\\nstate and action space design. The final dynamics can be\\nobtained by accumulating the vehicle position and head-\\ning from environment. The control process also mod-\\neled as a Markov decision process defined by Mv =\\n{Sv, Av, T v, Rv, γv}, with goal-conditioned RL for train-\\ning. Further details are provided below.\\nHistory-aware states. The VehExecutor incorporates\\nhistorical information into its states, alongside its current\\nstate, to improve the temporal consistency of the policy.\\nThe vehicle states consist of the planned trajectory segment\\nˆPv, temporal velocity Vv, and dynamic parametersΘv. Pv'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 5}, page_content='state, to improve the temporal consistency of the policy.\\nThe vehicle states consist of the planned trajectory segment\\nˆPv, temporal velocity Vv, and dynamic parametersΘv. Pv\\nis a slice of the planned trajectory in the vehicle’s coordi-\\nnate system over the current and adjacent τv frames. Vv\\nMethods Language command categoryWithin road User preferencesingle interaction compound\\nLCTGen [63]0.926 0.209 0.682 0.604 0.143\\nChatSim [70] 0.874 0.052 0.778 0.865 0.061\\nOurs 0.952 0.883 0.896 0.935 0.796\\nTable 1. High-level planning evaluation for command following\\nrate, within road rate and user preference rate.\\nrepresents the vehicle’s centroid velocity over τv frames.\\nΘv includes inherent vehicle parameters such as L (vehicle\\nlength), W (vehicle width), lf (front overhang) and lr (rear\\noverhang). These parameters provide prior information that\\ninfluence the physical transition process.\\nVehicle actions. To accurately simulate real vehicle\\nactions and maintain temporal consistency, the vehicle’s ac-\\ntion space Av is defined by the delta steering angle ∆δ and\\nscalar acceleration a, which are the two most direct controls\\naffecting vehicle movement in actual driving.\\nVehicle transition function. We employed the bicycle\\nmodel for modeling the vehicle physical transition process.\\nLet x and y denote the vehicle’s coordinates,v represent the\\nscalar velocity, andϕ indicate the vehicle’s orientation. Uti-\\nlizing the inherent parameters from the states, the vehicle’s'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 5}, page_content='Let x and y denote the vehicle’s coordinates,v represent the\\nscalar velocity, andϕ indicate the vehicle’s orientation. Uti-\\nlizing the inherent parameters from the states, the vehicle’s\\nstate transition process can be expressed as:\\n˙x = v ∗ cos(ϕ + β), ˙y = v ∗ sin(ϕ + β),\\nβ = arctan( lr\\nlf + lr\\ntan(δ)), ˙ϕ = v\\nlf + lr\\ncos(β)tan(δ).\\nWhere β denotes tire slip angle and ˙ denotes derivation.\\nThe bicycle model effectively simulates the physical state\\nchanges of the vehicle as determined by vehicle actions.\\nReward and training. The reward focuses on two key\\naspects: following the planned trajectory and the smooth-\\nness of results. Therefore, the reward consists of two com-\\nponents: Rv\\npos = −||ˆpt − pt||2 for following the planned\\ntrajectory and Rv\\nact = −(||∆δ||2 + ||a||2) for smooth driv-\\ning. Training uses TD3 [20] to maximize the accumulated\\ndiscounted reward. Note that: (i) actions can be further\\nsmoothed with temporal filtering: Av\\nt = αAv\\nt−1 + (1 −\\nα)Av\\npolicy, where At is the action taken at timestep t and\\nApolicy is the action directly output from the policy net-\\nwork; (ii) obstacles can be considered by concatenating\\ntheir positions and radius in the state and addingRv\\nobs = ϵ\\nLo\\nto the reward ifLo < D, where Lo is the distance to the ob-\\nstacle, ϵ is a coefficient and D is a threshold.\\n4. Experiments\\n4.1. Implementation Details and Dataset\\nFor the PedExecutor, we utilize Isaac Gym [47] as the\\nphysics simulation engine, with the AMASS dataset [46]'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 5}, page_content='4. Experiments\\n4.1. Implementation Details and Dataset\\nFor the PedExecutor, we utilize Isaac Gym [47] as the\\nphysics simulation engine, with the AMASS dataset [46]\\nserving as the reference data clip for the discrimination re-\\nward. A model of the SMPL [42] robot as the simulation\\n6'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 6}, page_content='31帧\\n90帧\\nFRAME 1\\nFRAME 3\\nCommand: A person is taking a taxi at the left roadside, and a vehicle \\novertakes the taxi. Two persons are walking together with one’ s arm around \\nanother’ s shoulder. A person is chasing another person along the roadside.\\nFRAME 2\\nFRAME 3 w. rendering\\n49帧\\n63帧\\nFRAME 1\\nFRAME 2\\nFRAME 3\\nFRAME 3 w. rendering\\nCommand: A person pushes another person, and another person is \\nmaking a phone call, then walks along the roadside. A vehicle turns right \\nat the intersection, and a hurried vehicle overtakes a stationary one.\\nFigure 5. System results under complex and composite commands, with diverse interaction information and realistic dynamics output.\\nLCTGen\\nOurs\\nOurs with \\npedestrian\\nCommand: Three vehicles are moving forawrd. \\nAnother vehicle is overtaking and one vehicle is \\npulling over.\\nCommand: A vehicle is turning left. T wo \\nvehicles are moving straight.  And another \\nvehicle stops at roadside.\\nCommand: A person is crossing the road to get \\ninto a vehicle. Some people are walking along the \\nroadside.  A person is taking a taxi.  A vehicle \\novertakes another, and a vehicle follows a person.\\nCommand: Some vehicles are at the intersection.  \\nA person crosses the road towards a stationary\\nvehicle. Some people walk at a region and another \\nvehicle drives slowly into the region.\\nFigure 6. High-level planning results comparison under vehicle-\\nonly command, and our planning results with pedestrians involved.'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 6}, page_content='vehicle drives slowly into the region.\\nFigure 6. High-level planning results comparison under vehicle-\\nonly command, and our planning results with pedestrians involved.\\nThe boxes indicate vehicles and the circles indicate pedestrians.\\nobject. For the VehExecutor, we construct a simulation\\nenvironment incorporating physical transitions. All LLM\\ncomponents used in the LLM-agents are implemented with\\nGPT-4 [1] API. For the final rendering, we followed the\\nrendering pipeline in ChatSim [70], using Blender for ren-\\ndering with SMPL based human rendering implementation.\\nDetailed experimental settings, configurations and video re-\\nsults are provided in the supplementary materials.\\n4.2. System Results\\nWe present keyframes from the output of the system in\\ntwo scenes, as shown in Figure 5. Each generated output\\nincludes abundant traffic participants based on user com-\\nmand. From the system’s outputs, we see that: (i) inter-\\naction is thoroughly depicted, including human-vehicle in-\\na. PPO b. PULSE\\nc. AMP d. Ours\\nPedestrian dynamics of pushingtask \\nFigure 7. Visualization comparison of interaction task execution.\\nteractions (e.g. taking a taxi, avoidance, slowing down),\\nvehicle-vehicle interactions (e.g. lane changes, overtak-\\ning), and human-human interactions (e.g. pushing, chas-\\ning, walking with arm around shoulder). These interactions\\namong different types of traffic participants contribute to a\\nmore diverse and realistic scene dynamics; (ii) the system'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 6}, page_content='ing, walking with arm around shoulder). These interactions\\namong different types of traffic participants contribute to a\\nmore diverse and realistic scene dynamics; (ii) the system\\nachieves precise and detailed control, with each scene de-\\nscription being complex, potentially including abstract se-\\nmantics. Through the design of multi-LLM-agents role-\\nplaying, complex instructions are accurately analyzed and\\nexecuted, and abstract semantic information is effectively\\nbroken down into executable commands for final genera-\\ntion; (iii) the generated results are highly realistic. The\\nphysics-based control policies for pedestrians and vehicles\\nproduce realistic, and intuitive dynamics, particularly high-\\nlighting physical feedback in human-human interaction.\\n7'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 7}, page_content='Methods FID↓ Div.↑ l-FID↓ l-Div.↑ Empjpe↓ Ef↓ UP↑\\nPacer[56] 7.25 1.24 7.93 1.05 \\\\ 0.122 0.179\\nPacer+[69] 6.62 1.58 7.76 1.28 82.33 0.128 0.289\\nOurs 6.21 1.76 7.07 1.49 79.82 0.124 0.532\\nTable 2. Dynamics quality, diversity and accuracy evaluation for\\ntrajectory following and motion imitation tasks.\\nMethods Unified policy Interaction 1 Interaction 2 Interaction 3\\nPPO [57] × 0.971 0.934 0.914\\nAMP [51] × 0.234 0.179 0.108\\nPulse [45] × 0.975 0.942 0.925\\nOurs ✓ 0.982 0.977 0.971\\nTable 3. Interaction tasks success rate evaluation.\\n4.3. Component Results\\n4.3.1. LLM-agents planning\\nWe compare the planned trajectory generated by LLM\\nagents with existing traffic generation methods based on\\nlanguage, including LCTGen [63] and ChatSim [70]. Since\\nother methods do not support scenarios involving pedestri-\\nans, we conduct an evaluation under vehicle-only instruc-\\ntions. We define three types of instructions: single-vehicle\\ninstructions, interaction instructions, and composite instruc-\\ntions. Using 5 maps, we generate 26 samples per instruction\\ntype and invite 10 users to evaluate each generated result.\\nThey assess whether each result matches the language de-\\nscription, whether it is within-road, and their preference for\\neach sample. The proportion of positive responses for each\\ncriterion is summarized in Table 1. The results indicate that\\nChatDyn consistently produced more accurate planning re-\\nsults that matched descriptions and were preferred by users.'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 7}, page_content='criterion is summarized in Table 1. The results indicate that\\nChatDyn consistently produced more accurate planning re-\\nsults that matched descriptions and were preferred by users.\\nWe also provide visual examples in Figure 6, which also in-\\nvolves some planning results of ChatDyn with pedestrians.\\nLCTGen struggles with interaction controls and complex\\ninstructions, while ChatDyn accurately fulfill all require-\\nments, producing high-quality outputs that meet specifica-\\ntions, even in scenarios involving pedestrians.\\n4.3.2. Pedestrian executor\\nWe follow the evaluation from Pacer+ [69] to assess tra-\\njectory following and motion imitation tasks for PedExecu-\\ntor as shown in Table 2. The quality and diversity of the\\ngenerated results are measured by Frechet Inception Dis-\\ntance (FID) [27] and diversity metric (Div.) [69] on nor-\\nmal speed, while l-FID and l-Div. are the metrics on low\\nspeed; imitation accuracy by Mean Per-Joint Position Error\\n(Empjpe), and following accuracy by following error (Ef ).\\nAdditionally, user preference (UP) is assessed by inviting\\n15 users to evaluate 33 segments of dynamics. With the\\nsupport of hierarchical control and related training strategy,\\nPedExecutor generates higher quality dynamics and demon-\\nstrated highly competitive performance in both following\\nand imitation tasks.\\nWe also evaluate the process of performing interaction\\nTE HC M. AMP Success rate User preference\\n× ✓ ✓ 0.403 0.120\\n✓ ✓ × 0.186 0.069\\n✓ × ✓ 0.948 0.344\\n✓ ✓ ✓ 0.977 0.467'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 7}, page_content='and imitation tasks.\\nWe also evaluate the process of performing interaction\\nTE HC M. AMP Success rate User preference\\n× ✓ ✓ 0.403 0.120\\n✓ ✓ × 0.186 0.069\\n✓ × ✓ 0.948 0.344\\n✓ ✓ ✓ 0.977 0.467\\nTable 4. PedExecutor interaction task ablation study.\\na. Without obstacles b. With obstacles \\nFigure 8. Comparison of vehicle dynamics generation. Green\\nboxes are our results and red boxes are Xu et al.’s [77]. Yellow\\nlines are the planned trajectory and blue circles are obstacles.\\ntasks to determine each method’s effectiveness in complet-\\ning these tasks. Note that PedExecutor is a unified policy\\nhandling all tasks, while other methods rely on separately\\ntrained policies for each specific interaction task. Three\\ntypes of interaction tasks are selected for training and vali-\\ndation: pushing (interaction 1), patting (interaction 2), and\\nwalking with an arm around the shoulder (interaction 3).\\nThe experimental results are presented in Table 3, with vi-\\nsual results shown in Figure 7. From the both results, we see\\nthat, AMP [51] generates dynamics resembling the refer-\\nence motion, but struggles to access suitable reference data\\nclips for interaction tasks, resulting in actions that only su-\\nperficially mimic the reference without fulfilling the task\\nrequirements. PPO [57] and PULSE [45] demonstrate rel-\\natively high task success rates, but the visualizations show\\nthat their outputs lack human-like style and appear unnatu-\\nral. In contrast, with hierarchical control and body masked'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 7}, page_content='atively high task success rates, but the visualizations show\\nthat their outputs lack human-like style and appear unnatu-\\nral. In contrast, with hierarchical control and body masked\\nAMP, PedExecutor achieves both high success rates and\\nmaintains high-quality, human-like outputs.\\nAs shown in Table 4, we also conduct ablation studies\\non interaction tasks to verify the effectiveness of task em-\\nbedding (TE), hierarchical control (HC), and body masked\\nAMP (M. AMP). The results indicate that, without task em-\\nbedding, PedExecutor struggles to accurately identify the\\nrequired interaction task, leading to difficulty in success-\\nful task completion. When body masked AMP is removed,\\nsimilar to AMP, the discrimination reward lacks a task-\\ncompleting reference. In this situation, the outcomes can\\nmerely be close to the reference data clip without the ability\\nto successfully execute the intended task. Without hierar-\\nchical control, the absence of action priors slows the train-\\ning process and reduces the success rates. More video com-\\nparison results can be found in the supplementary materials.\\n4.3.3. Vehicle executor\\nWe evaluate the accuracy of VehExecutor under varying\\ninitial speeds by measuring the position error and velocity\\n8'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 8}, page_content='Methods/Speed 0 5 10 20\\nPP [10] 0.129/0.103 0.143/0.125 0.162/0.142 0.231/0.208\\nXu et al. [77] 0.075/0.054 0.084/0.066 0.095/0.077 0.138/0.114\\nOurs 0.059/0.037 0.062 /0.041 0.077 /0.054 0.106 /0.088\\nTable 5. Position/velocity error of vehicle dynamics generation.\\nFilt. Action spa. His. state Position error Velocity error\\n× × × 0.138 0.114\\n✓ × × 0.132 0.111\\n✓ ✓ × 0.115 0.092\\n✓ ✓ ✓ 0.106 0.088\\nTable 6. VehExecutor ablation study.\\nerror, comparing our approach with Xu et al.’s method [77]\\nand pure pursuit [10] (PP) in Table 5. VehExecutor consis-\\ntently achieves the best performance across all speeds. We\\nalso conduct an ablation study in Table 6, examining the\\neffects of action filtering (Filt.), the composition of the ac-\\ntion space (Action spa.) which means it consists directly of\\nvelocity and steering or their variations, and the inclusion\\nof historical state (His. state) information. The ablation\\nresults validate the effectiveness of each design. Addition-\\nally, visual experiments in Figure 8 further demonstrate our\\nmethod achieves more precise tracking, and obstacle avoid-\\nance capabilities in scenarios with obstacles.\\n5. Conclusion and Limitations\\nWe propose ChatDyn, the first system to achieve inter-\\nactive and realistic language-driven multi-actor dynamics\\ngeneration in street scenes. ChatDyn utilizes multi-LLM-\\nagent role-playing to enable interaction-aware high-level\\nplanning under instructions. We introduce PedExecutor, a'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 8}, page_content='generation in street scenes. ChatDyn utilizes multi-LLM-\\nagent role-playing to enable interaction-aware high-level\\nplanning under instructions. We introduce PedExecutor, a\\nunified control policy for realistic pedestrian dynamics gen-\\neration across multiple tasks, also enabling fine-grained in-\\nteractions. We introduce VehExecutor, a physics-based ve-\\nhicle control policy with a history-aware design to gener-\\nate realistic vehicle dynamics. The dynamics output can\\nbe used as a prior for various kinds of simulators including\\nrenderer, reconstruction, and generation model.\\nIn the future , we plan to integrate more types\\nof dynamics with diverse characteristics, such as cy-\\nclists and certain common animals, to reflect more\\nrealistic and varied scene dynamics. At the same\\ntime, more complex and diverse interaction behaviors\\ncan be further explored to construct scene informa-\\ntion and events with greater accuracy. The relevant\\nmethodologies, after specific optimizations, can also be\\napplied to more closed, yet finer-grained indoor scenes.\\nReferences\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\\nGpt-4 technical report. arXiv preprint arXiv:2303.08774 ,\\n2023. 3, 7\\n[2] Harley Amado, Sara Ferreira, Jose Pedro Tavares, Paulo\\nRibeiro, and Elisabete Freitas. Pedestrian–vehicle interac-\\ntion at unsignalized crosswalks: a systematic review. Sus-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 8}, page_content='2023. 3, 7\\n[2] Harley Amado, Sara Ferreira, Jose Pedro Tavares, Paulo\\nRibeiro, and Elisabete Freitas. Pedestrian–vehicle interac-\\ntion at unsignalized crosswalks: a systematic review. Sus-\\ntainability, 12(7):2805, 2020. 1\\n[3] Nikos Athanasiou, Mathis Petrovich, Michael J Black, and\\nG¨ul Varol. Teach: Temporal action composition for 3d hu-\\nmans. In 2022 International Conference on 3D Vision (3DV),\\npages 414–423. IEEE, 2022. 2\\n[4] Jinseok Bae, Jungdam Won, Donggeun Lim, Cheol-Hui Min,\\nand Young Min Kim. Pmp: Learning to physically inter-\\nact with environments using part-wise motion priors. In\\nACM SIGGRAPH 2023 Conference Proceedings , pages 1–\\n10, 2023. 2\\n[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\\nHuang, et al. Qwen technical report. arXiv preprint\\narXiv:2309.16609, 2023. 3\\n[6] Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih\\nHu, Luca Del Pero, Bła˙zej Osi´nski, Hugo Grimmett, and Pe-\\nter Ondruska. Simnet: Learning reactive self-driving simu-\\nlations from real-world observations. In 2021 IEEE Inter-\\nnational Conference on Robotics and Automation (ICRA) ,\\npages 5119–5125. IEEE, 2021. 3\\n[7] Michael J Black, Priyanka Patel, Joachim Tesch, and Jin-\\nlong Yang. Bedlam: A synthetic dataset of bodies exhibit-\\ning detailed lifelike animated motion. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 8726–8737, 2023. 2'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 8}, page_content='ing detailed lifelike animated motion. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 8726–8737, 2023. 2\\n[8] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-\\nmodal dataset for autonomous driving. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 11621–11631, 2020. 1\\n[9] He Chen, Hongpinng Ren, Rui Li, Guang Yang, and Shan-\\nshan Ma. Generating autonomous driving test scenarios\\nbased on openscenario. In 2022 9th International Confer-\\nence on Dependable Systems and Their Applications (DSA),\\npages 650–658. IEEE, 2022. 3\\n[10] R Craig Coulter. Implementation of the pure pursuit path\\ntracking algorithm. Technical report, DTIC Document, 1992.\\n9, 3\\n[11] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav\\nGolyanik, and Christian Theobalt. Mofusion: A framework\\nfor denoising-diffusion-based motion synthesis. In Proceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 9760–9770, 2023. 2\\n[12] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu,\\nBo Dai, and Yansong Tang. Motionlcm: Real-time control-\\nlable motion generation via latent consistency model. arXiv\\npreprint arXiv:2404.19759, 2024. 2\\n[13] Wenhao Ding, Yulong Cao, Ding Zhao, Chaowei Xiao,\\nand Marco Pavone. Realgen: Retrieval augmented gen-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 8}, page_content='preprint arXiv:2404.19759, 2024. 2\\n[13] Wenhao Ding, Yulong Cao, Ding Zhao, Chaowei Xiao,\\nand Marco Pavone. Realgen: Retrieval augmented gen-\\neration for controllable traffic scenarios. arXiv preprint\\narXiv:2312.13303, 2023. 3\\n9'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 9}, page_content='[14] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-\\nnio Lopez, and Vladlen Koltun. Carla: An open urban driv-\\ning simulator. In Conference on robot learning, pages 1–16.\\nPMLR, 2017. 1\\n[15] Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura,\\nand Wenping Wang. C· ase: Learning conditional adversar-\\nial skill embeddings for physics-based characters. In SIG-\\nGRAPH Asia 2023 Conference Papers, pages 1–11, 2023. 2\\n[16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\\nllama 3 herd of models. arXiv preprint arXiv:2407.21783 ,\\n2024. 3\\n[17] Lan Feng, Quanyi Li, Zhenghao Peng, Shuhan Tan, and\\nBolei Zhou. Trafficgen: Learning to generate diverse and re-\\nalistic traffic scenarios. In 2023 IEEE International Confer-\\nence on Robotics and Automation (ICRA), pages 3567–3575.\\nIEEE, 2023. 3\\n[18] Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka\\nPatel, and Michael J. Black. ChatPose: Chatting about 3D\\nhuman pose. In CVPR, 2024. 3\\n[19] Daniel J Fremont, Tommaso Dreossi, Shromona Ghosh, Xi-\\nangyu Yue, Alberto L Sangiovanni-Vincentelli, and Sanjit A\\nSeshia. Scenic: a language for scenario specification and\\nscene generation. In Proceedings of the 40th ACM SIGPLAN\\nconference on programming language design and implemen-\\ntation, pages 63–78, 2019. 3\\n[20] Scott Fujimoto, Herke Hoof, and David Meger. Address-\\ning function approximation error in actor-critic methods. In'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 9}, page_content='tation, pages 63–78, 2019. 3\\n[20] Scott Fujimoto, Herke Hoof, and David Meger. Address-\\ning function approximation error in actor-critic methods. In\\nInternational conference on machine learning, pages 1587–\\n1596. PMLR, 2018. 6\\n[21] Luiz G Galv ˜ao and M Nazmul Huda. Pedestrian and vehicle\\nbehaviour prediction in autonomous vehicle system—a re-\\nview. Expert Systems with Applications, 238:121983, 2024.\\n1\\n[22] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui\\nZhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng,\\nHanlin Zhao, et al. Chatglm: A family of large language\\nmodels from glm-130b to glm-4 all tools. arXiv preprint\\narXiv:2406.12793, 2024. 3\\n[23] Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dauten-\\nhahn, and Nasser Lashgarian Azad. Pedestrian trajectory\\nprediction in pedestrian-vehicle mixed environments: A sys-\\ntematic review. IEEE Transactions on Intelligent Trans-\\nportation Systems, 2023. 1\\n[24] Mahir Gulzar, Yar Muhammad, and Naveed Muhammad. A\\nsurvey on motion prediction of pedestrians and vehicles for\\nautonomous driving. IEEE Access, 9:137957–137969, 2021.\\n1\\n[25] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,\\nXingyu Li, and Li Cheng. Generating diverse and natural 3d\\nhuman motions from text. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 5152–5161, 2022. 2\\n[26] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen\\nWang, and Li Cheng. Momask: Generative masked model-\\ning of 3d human motions. In Proceedings of the IEEE/CVF'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 9}, page_content='pages 5152–5161, 2022. 2\\n[26] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen\\nWang, and Li Cheng. Momask: Generative masked model-\\ning of 3d human motions. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 1900–1910, 2024. 2, 5\\n[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\\ntwo time-scale update rule converge to a local nash equilib-\\nrium. Advances in neural information processing systems ,\\n30, 2017. 8\\n[28] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng,\\nJinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing\\nYau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta pro-\\ngramming for multi-agent collaborative framework. arXiv\\npreprint arXiv:2308.00352, 2023. 3\\n[29] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong\\nWang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu\\nWang. Large language models for software engineering: A\\nsystematic literature review. ACM Transactions on Software\\nEngineering and Methodology, 2023. 3\\n[30] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models. arXiv\\npreprint arXiv:2106.09685, 2021. 3\\n[31] Stefan Jesenski, Jan Erik Stellet, Florian Schiegg, and J Mar-\\nius Z¨ollner. Generation of scenes in intersections for the val-\\nidation of highly automated driving functions. In 2019 IEEE'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 9}, page_content='[31] Stefan Jesenski, Jan Erik Stellet, Florian Schiegg, and J Mar-\\nius Z¨ollner. Generation of scenes in intersections for the val-\\nidation of highly automated driving functions. In 2019 IEEE\\nIntelligent Vehicles Symposium (IV), pages 502–509. IEEE,\\n2019. 3\\n[32] Jordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin\\nPeng. Padl: Language-directed physics-based character con-\\ntrol. In SIGGRAPH Asia 2022 Conference Papers , pages\\n1–9, 2022. 2\\n[33] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-\\nform language-based motion synthesis & editing. In Pro-\\nceedings of the AAAI Conference on Artificial Intelligence ,\\npages 8255–8263, 2023. 2\\n[34] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur,\\nMing Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan\\nZhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwe-\\nbarena: Evaluating multimodal agents on realistic visual web\\ntasks. arXiv preprint arXiv:2401.13649, 2024. 3\\n[35] Yan Leng and Yuan Yuan. Do llm agents exhibit social be-\\nhavior? arXiv preprint arXiv:2312.15198, 2023. 3\\n[36] Binxu Li, Tiankai Yan, Yuanting Pan, Jie Luo, Ruiyang Ji,\\nJiayuan Ding, Zhe Xu, Shilong Liu, Haoyu Dong, Zihao Lin,\\net al. Mmedagent: Learning to use medical tools with multi-\\nmodal agent. arXiv preprint arXiv:2407.02483, 2024. 3\\n[37] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei\\nLai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent hospi-\\ntal: A simulacrum of hospital with evolvable medical agents.\\narXiv preprint arXiv:2405.02957, 2024. 3'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 9}, page_content='Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent hospi-\\ntal: A simulacrum of hospital with evolvable medical agents.\\narXiv preprint arXiv:2405.02957, 2024. 3\\n[38] Weiwei Li and Emanuel Todorov. Iterative linear quadratic\\nregulator design for nonlinear biological movement systems.\\nIn First International Conference on Informatics in Control,\\nAutomation and Robotics, pages 222–229. SciTePress, 2004.\\n3\\n[39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. Advances in neural information\\nprocessing systems, 36, 2024. 3\\n10'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 10}, page_content='[40] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan\\nXue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo,\\nSongyou Peng, Yandong Wen, Michael J. Black, Adrian\\nWeller, and Bernhard Sch¨olkopf. Parameter-efficient orthog-\\nonal finetuning via butterfly factorization. In ICLR, 2024.\\n3\\n[41] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei,\\nHanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan\\nYang, et al. Agentbench: Evaluating llms as agents. arXiv\\npreprint arXiv:2308.03688, 2023. 3\\n[42] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\\nperson linear model. In Seminal Graphics Papers: Pushing\\nthe Boundaries, Volume 2, pages 851–866, 2023. 6, 2\\n[43] Jack Lu, Kelvin Wong, Chris Zhang, Simon Suo, and Raquel\\nUrtasun. Scenecontrol: Diffusion for controllable traffic\\nscene generation. In 2024 IEEE International Conference\\non Robotics and Automation (ICRA) , pages 16908–16914.\\nIEEE, 2024. 3\\n[44] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al.\\nPerpetual humanoid control for real-time simulated avatars.\\nIn Proceedings of the IEEE/CVF International Conference\\non Computer Vision, pages 10895–10904, 2023. 2, 1\\n[45] Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler,\\nJing Huang, Kris Kitani, and Weipeng Xu. Universal hu-\\nmanoid motion representations for physics-based control.\\narXiv preprint arXiv:2310.04582, 2023. 2, 5, 8\\n[46] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 10}, page_content='manoid motion representations for physics-based control.\\narXiv preprint arXiv:2310.04582, 2023. 2, 5, 8\\n[46] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-\\nard Pons-Moll, and Michael J Black. Amass: Archive\\nof motion capture as surface shapes. In Proceedings of\\nthe IEEE/CVF international conference on computer vision,\\npages 5442–5451, 2019. 6\\n[47] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,\\nMichelle Lu, Kier Storey, Miles Macklin, David Hoeller,\\nNikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac\\ngym: High performance gpu-based physics simulation for\\nrobot learning. arXiv preprint arXiv:2108.10470, 2021. 6\\n[48] Ying Ni, Menglong Wang, Jian Sun, and Keping Li. Eval-\\nuation of pedestrian safety at intersections: A theoretical\\nframework based on pedestrian-vehicle interaction patterns.\\nAccident Analysis & Prevention, 96:118–129, 2016. 1\\n[49] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\\nmodels to follow instructions with human feedback. Ad-\\nvances in neural information processing systems, 35:27730–\\n27744, 2022. 3\\n[50] Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun\\nZhang, Yanfeng Wang, and Siheng Chen. Self-alignment\\nof large language models via monopolylogue-based social\\nscene simulation. arXiv preprint arXiv:2402.05699, 2024. 3\\n[51] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 10}, page_content='of large language models via monopolylogue-based social\\nscene simulation. arXiv preprint arXiv:2402.05699, 2024. 3\\n[51] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and\\nAngjoo Kanazawa. Amp: Adversarial motion priors for styl-\\nized physics-based character control. ACM Transactions on\\nGraphics (ToG), 40(4):1–20, 2021. 2, 4, 5, 8\\n[52] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine,\\nand Sanja Fidler. Ase: Large-scale reusable adversarial\\nskill embeddings for physically simulated characters. ACM\\nTransactions On Graphics (TOG), 41(4):1–17, 2022. 2, 1\\n[53] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao\\nFeng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard\\nSch¨olkopf. Controlling text-to-image diffusion by orthogo-\\nnal finetuning. In NeurIPS, 2023. 3\\n[54] Rodrigo Queiroz, Thorsten Berger, and Krzysztof Czarnecki.\\nGeoscenario: An open dsl for autonomous driving scenario\\nrepresentation. In 2019 IEEE Intelligent Vehicles Symposium\\n(IV), pages 287–294. IEEE, 2019. 3\\n[55] Davis Rempe, Jonah Philion, Leonidas J Guibas, Sanja Fi-\\ndler, and Or Litany. Generating useful accident-prone driv-\\ning scenarios via a learned traffic prior. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 17305–17315, 2022. 3\\n[56] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris\\nKitani, Karsten Kreis, Sanja Fidler, and Or Litany. Trace and\\npace: Controllable pedestrian animation via guided trajec-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 10}, page_content='[56] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris\\nKitani, Karsten Kreis, Sanja Fidler, and Or Litany. Trace and\\npace: Controllable pedestrian animation via guided trajec-\\ntory diffusion. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages 13756–\\n13766, 2023. 1, 2, 8\\n[57] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\\nford, and Oleg Klimov. Proximal policy optimization algo-\\nrithms. arXiv preprint arXiv:1707.06347, 2017. 5, 8\\n[58] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H\\nBermano. Human motion diffusion as a generative prior.\\narXiv preprint arXiv:2303.01418, 2023. 2\\n[59] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\\nWeiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai\\ntasks with chatgpt and its friends in hugging face. Advances\\nin Neural Information Processing Systems, 36, 2024. 3\\n[60] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\\nYuning Chai, Benjamin Caine, et al. Scalability in perception\\nfor autonomous driving: Waymo open dataset. In Proceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 2446–2454, 2020. 1, 3\\n[61] Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel\\nUrtasun. Trafficsim: Learning to simulate realistic multi-\\nagent behaviors. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , pages\\n10400–10409, 2021. 3'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 10}, page_content='Urtasun. Trafficsim: Learning to simulate realistic multi-\\nagent behaviors. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , pages\\n10400–10409, 2021. 3\\n[62] Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Mani-\\nvasagam, Mengye Ren, and Raquel Urtasun. Scenegen:\\nLearning to generate realistic traffic scenes. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 892–901, 2021. 3\\n[63] Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone,\\nand Philipp Kraehenbuehl. Language conditioned traffic\\ngeneration. arXiv preprint arXiv:2307.07947 , 2023. 1, 3,\\n6, 8, 4\\n[64] Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang,\\nHongyu Zhang, and Yu Cheng. Magis: Llm-based multi-\\nagent framework for github issue resolution. arXiv preprint\\narXiv:2403.17927, 2024. 3\\n[65] Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal\\nChechik, and Xue Bin Peng. Calm: Conditional adversarial\\n11'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 11}, page_content='latent models for directable virtual characters. In ACM SIG-\\nGRAPH 2023 Conference Proceedings, pages 1–9, 2023. 2\\n[66] Sourabh Thakur and Subhadip Biswas. Assessment of\\npedestrian-vehicle interaction on urban roads: a critical re-\\nview. Archives of transport, 51, 2019. 1\\n[67] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\\nLlama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971, 2023. 3\\n[68] Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang,\\nDinesh Jayaraman, and Lingjie Liu. Tlcontrol: Trajectory\\nand language control for human motion synthesis. arXiv\\npreprint arXiv:2311.17135, 2023. 2\\n[69] Jingbo Wang, Zhengyi Luo, Ye Yuan, Yixuan Li, and Bo\\nDai. Pacer+: On-demand pedestrian animation controller in\\ndriving scenarios. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n718–728, 2024. 1, 2, 5, 8\\n[70] Yuxi Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing\\nLiu, Hao Zhao, Siheng Chen, and Yanfeng Wang. Editable\\nscene simulation for autonomous driving via collaborative\\nllm-agents. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages 15077–\\n15087, 2024. 6, 7, 8, 2, 4\\n[71] Jungdam Won, Deepak Gopinath, and Jessica Hodgins.\\nPhysics-based character controllers using conditional vaes.\\nACM Transactions on Graphics (TOG), 41(4):1–12, 2022. 2'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 11}, page_content='15087, 2024. 6, 7, 8, 2, 4\\n[71] Jungdam Won, Deepak Gopinath, and Jessica Hodgins.\\nPhysics-based character controllers using conditional vaes.\\nACM Transactions on Graphics (TOG), 41(4):1–12, 2022. 2\\n[72] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun\\nZhang, and Chi Wang. Autogen: Enabling next-gen llm ap-\\nplications via multi-agent conversation framework. arXiv\\npreprint arXiv:2308.08155, 2023. 3\\n[73] Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang,\\nXiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun,\\nKun Jiang, et al. Pandaset: Advanced sensor suite dataset\\nfor autonomous driving. In 2021 IEEE International In-\\ntelligent Transportation Systems Conference (ITSC) , pages\\n3095–3101. IEEE, 2021. 1\\n[74] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and\\nHuaizu Jiang. Omnicontrol: Control any joint at any time for\\nhuman motion generation. arXiv preprint arXiv:2310.08580,\\n2023. 2\\n[75] Pei Xu, Xiumin Shang, Victor Zordan, and Ioannis\\nKaramouzas. Composite motion learning with task control.\\nACM Transactions on Graphics (TOG), 42(4):1–16, 2023. 2\\n[76] Pei Xu, Kaixiang Xie, Sheldon Andrews, Paul G Kry,\\nMichael Neff, Morgan McGuire, Ioannis Karamouzas, and\\nVictor Zordan. Adaptnet: Policy adaptation for physics-\\nbased character control. ACM Transactions on Graphics\\n(TOG), 42(6):1–17, 2023. 2\\n[77] Yinda Xu and Lidong Yu. Drl-based trajectory tracking\\nfor motion-related modules in autonomous driving. arXiv'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 11}, page_content='based character control. ACM Transactions on Graphics\\n(TOG), 42(6):1–17, 2023. 2\\n[77] Yinda Xu and Lidong Yu. Drl-based trajectory tracking\\nfor motion-related modules in autonomous driving. arXiv\\npreprint arXiv:2308.15991, 2023. 8, 9, 3\\n[78] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce\\nBian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan,\\net al. Baichuan 2: Open large-scale language models. arXiv\\npreprint arXiv:2309.10305, 2023. 3\\n[79] Heyuan Yao, Zhenhua Song, Baoquan Chen, and Libin Liu.\\nControlvae: Model-based learning of generative controllers\\nfor physics-based characters. ACM Transactions on Graph-\\nics (TOG), 41(6):1–16, 2022. 2\\n[80] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou\\nHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-\\nfuse: Text-driven human motion generation with diffusion\\nmodel. arXiv preprint arXiv:2208.15001, 2022. 2\\n[81] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,\\nFangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Re-\\nmodiffuse: Retrieval-augmented motion diffusion model. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 364–373, 2023. 2\\n[82] Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic,\\nYulong Cao, Danfei Xu, Marco Pavone, and Baishakhi Ray.\\nLanguage-guided traffic simulation via scene-level diffusion.\\nIn Conference on Robot Learning , pages 144–177. PMLR,\\n2023. 1, 3\\n[83] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen,\\nSushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone.'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 11}, page_content='In Conference on Robot Learning , pages 144–177. PMLR,\\n2023. 1, 3\\n[83] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen,\\nSushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone.\\nGuided conditional diffusion for controllable traffic simula-\\ntion. In 2023 IEEE International Conference on Robotics\\nand Automation (ICRA), pages 3560–3566. IEEE, 2023. 3\\n[84] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert\\nLo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan\\nBisk, Daniel Fried, et al. Webarena: A realistic web en-\\nvironment for building autonomous agents. arXiv preprint\\narXiv:2307.13854, 2023. 3\\n[85] Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng\\nLiao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura,\\nWenping Wang, and Lingjie Liu. Emdm: Efficient motion\\ndiffusion model for fast and high-quality motion generation.\\nIn European Conference on Computer Vision, pages 18–38.\\nSpringer, 2025. 2\\n12'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 12}, page_content='ChatDyn: Language-Driven Multi-Actor Dynamics Generation\\nin Street Scenes\\nSupplementary Material\\nS1. Details of PedExecutor\\nS1.1. PedExecutor State Details\\nThe components of the humanoid’s proprioception Sp\\nprop\\nare as follows: joint positions j ∈ R24×3, rotations q ∈\\nR24×6, linear velocities v ∈ R24×3, and angular veloci-\\nties ω ∈ R24×3 [69]. These components are normalized\\nwith respect to the agent’s heading and root position in our\\nsimulator. The rotation q is represented using a 6-degree-\\nof-freedom rotation representation. Sp\\nprop along with the\\ntrajectory state Sp\\ntraj, the motion state Sp\\nmo to be mimicked,\\nand the target state Sp\\ntar for interaction, collectively forms\\nthe complete observation. During the task masking pro-\\ncess, the remaining relevant states are masked by multiply-\\ning them by 0 based on the task to be executed, and in the\\nmotion state, specific joints can also be masked by multiply-\\ning them by 0 if necessary. For example, in the experiment,\\nonly the upper-body related states were retained.\\nS1.2. Task-related Reward and Training Details\\nReward designs. For trajectory following tasks [56],\\nthe reward is defined as Rtrajectory = e−2∥ˆpt−pt∥, where\\nˆpt is the position to followed at t, pt is the current\\ncharacter position. For motion imitation tasks [44],\\nRimitation = e−100||ˆjt\\npos−jt\\npos||⊙mt\\n+ e−10||ˆjt\\nrot−jt\\nrot||⊙mt\\n+\\ne−0.1||ˆjt\\nvel−jt\\nvel||⊙mt\\n+ e−0.1||ˆjt\\nω−jt\\nω||⊙mt\\n, where ˆindicates'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 12}, page_content='character position. For motion imitation tasks [44],\\nRimitation = e−100||ˆjt\\npos−jt\\npos||⊙mt\\n+ e−10||ˆjt\\nrot−jt\\nrot||⊙mt\\n+\\ne−0.1||ˆjt\\nvel−jt\\nvel||⊙mt\\n+ e−0.1||ˆjt\\nω−jt\\nω||⊙mt\\n, where ˆindicates\\nthe motion states to be imitated,mt is the mask to select the\\njoints for imitation, which are the joints of upper body in our\\nexperiments. Different interaction tasks require distinct re-\\nward designs [52]; here, we present three sample tasks: for\\npushing, Rpushing = 1−uup·u, uup is the global up vector u*\\nis the target’s up vector; for patting, Rpatting = e−||prh−c||,\\nprh is the position of right hand and c is the target con-\\ntact position; and for walking while bending the shoulder,\\nRwalking bending = e−2∥ˆpt−pt∥ +e−||prh−c||, which combines\\nthe trajectory following and target position contacting. The\\nfinal reward is calculated as R = 0.5 · Rdisc + 0.5 · Rtask.\\nInteraction process. The specific execution process for\\nthe three interaction tasks can be understood as follows: i)\\npushing, where the goal is to push the interaction object\\nover, ii) patting, where the task is to gently tap a specific\\npart of the interaction object (e.g., the shoulder) without\\nknocking it over, and iii) walking with arm around another’s\\nshoulder, where the agent walks along a specific path while\\nkeeping the arm in contact with a specific location on the in-\\nteraction object (e.g., the shoulder). During the training of\\nthe interaction tasks, the interaction object is replaced with'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 12}, page_content='keeping the arm in contact with a specific location on the in-\\nteraction object (e.g., the shoulder). During the training of\\nthe interaction tasks, the interaction object is replaced with\\na box in the physical environment to facilitate more stable\\ntraining. However, during testing, the interaction object is\\nreplaced with another character in the simulation environ-\\nment, and physical collisions and interactions are present,\\nleading to the final output in the test phase.\\nFailure recovery for robustness. To enhance the ro-\\nbustness of PedExecutor, specifically to ensure that it can\\nhandle external disturbances without failing due to pertur-\\nbations caused by physical collisions, we incorporate re-\\ncovery during its training [44]. In this approach, the hu-\\nman pose is initialized in a collapsed or otherwise unstable\\nstanding state. Training is then conducted from this ini-\\ntial state, allowing the policy to learn how to recover from\\nfailure. This enables the policy to exhibit robustness when\\nresponding to physical collisions and interactions. Without\\nthis, the policy might fail to continue the action after even\\nminor disturbances.\\nS1.3. Network Architecture\\nAll relevant models, including the baselines, adopt the same\\nnetwork architecture, using an MLP as the policy network\\nwith hidden layers of 2048 and 1024 units. The final output\\nis directed to either the latent space or the PD control sig-\\nnal, depending on whether hierarchical control is employed.'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 12}, page_content='with hidden layers of 2048 and 1024 units. The final output\\nis directed to either the latent space or the PD control sig-\\nnal, depending on whether hierarchical control is employed.\\nThe remaining network components, such as the discrim-\\ninator, value network, control frequency (30Hz), and hy-\\nperparameters used for training, are consistent with those\\nadopted in Pacer+ [69]. All training and testing are con-\\nducted on an NVIDIA 4090.\\nS1.4. Evaluation Details\\nFollowing and imitation. For the following and imitation\\ntasks, we adopt the same computation methods as those\\nused in Pacer+ [69]. The calculations of FID and diversity\\nare performed using the same manual feature extraction ap-\\nproach as in Pacer+, with 1000 segments selected from the\\nAMASS dataset for FID computation. For the low-speed\\nl-FID and l-diversity, we also follow Pacer+ by testing on\\ninstances where the speed is less than 1 m/s.\\nInteraction tasks. For the three interaction-related\\ntasks: i) pushing, the success criterion is whether the ob-\\nject is pushed over within the specified timestep (with a tilt\\nalong the z-axis greater than 30°), and no part of the body\\nother than the hands is in contact with the target; ii) pat-\\nting, the success criterion is whether, within the specified\\ntimestep, the right hand is within 5 cm of the target’s spe-\\ncific location and remains there for at least 50 timesteps,\\n1'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 13}, page_content='Pacer+\\n Ours\\nKey frame 1 \\nKey frame 2 \\nKey frame 3 \\nImitation and following comparison\\nFigure S1. Comparison of imitation and following.\\nwith no other part of the body in contact with the target\\nexcept the hands; iii) walking with arm around another’s\\nshoulder, the success criterion is that the maximum devia-\\ntion from the reference trajectory is no greater than 10 cm,\\nand the right hand is within 5 cm of the target’s specific lo-\\ncation for at least 150 timesteps. In the interaction ablation\\nstudy, for the user study, we recruit 15 participants to test\\n30 dynamic sequences and select the ones they considered\\nto have the highest quality.\\nS1.5. Rendering Details\\nWe utilize the rendering pipeline from ChatSim [70], em-\\nploying Blender’s Cycles as the rendering engine. Back-\\nground rendering and HDRI lighting from ChatSim are\\napplied to the scene. The human model is based on\\nSMPL [42], with the corresponding mesh initialized, and\\nrendered using skin and clothing textures provided in the\\nBedlam dataset [7].\\nS1.6. Supplymentary Experiments\\nWe provide more comprehensive visualization results to\\ncompare different aspects of PedExecutor’s characteristics.\\nAll comparisons, along with dynamic results, can also be\\nobserved in the supplementary video material.\\nImitation and following comparison. As shown in\\nS1, we compared the visual effects of Pacer+ during the\\nimitation and following processes. It is evident that, under\\nthe influence of hierarchical control, PedExecutor enables'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 13}, page_content='S1, we compared the visual effects of Pacer+ during the\\nimitation and following processes. It is evident that, under\\nthe influence of hierarchical control, PedExecutor enables\\nsmoother and more seamless transitions between different\\ntasks—transitions that cannot be achieved with the priors\\nprovided by AMP.\\nResults of imitation during following. As shown in S2,\\nLook around\\n Wave hand\\nKey frame 1 \\nKey frame 2 \\nKey frame 3 \\nImitation during following\\nFigure S2. Imitation during following.\\nKey frame 1 \\nKey frame 2 \\nKey frame 3 \\nLook around\\n Wave hand\\nFigure S3. Failure of PriorMDM [58] with action specification\\nduring following.\\nwe also provide the results of PedExecutor performing both\\nimitation and following simultaneously. In the case of fol-\\nlowing a specific trajectory, the upper body performs ac-\\ntions such as looking around and waving, showcasing capa-\\nbility of PedExecutor to handle both imitation and following\\ntasks concurrently.\\nInteraction task comparison. As shown in S4 S5 S6,\\nwe compared the performance of AMP, PPO, PULSE, and\\nour method across three interaction tasks. It is still evi-\\ndent that while PPO and PULSE are able to complete the\\n2'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 14}, page_content='tasks under the given conditions, they fail to produce nat-\\nural and human-like results. AMP, on the other hand, can\\nonly approximate the reference in the discriminator (walk-\\ning or running), and is completely unable to complete the\\ntasks. In contrast, our PedExecutor successfully completes\\nthe tasks while generating natural and human-like results.\\nS1.7. Further Discussion of Kinematics Methods\\nFor kinematics-based methods, some approaches can\\nachieve following and motion specification, but they are\\ncompletely unable to handle interaction-related tasks. Addi-\\ntionally, for following and motion specification tasks, these\\nmethods often suffer from overfitting to the dataset, leading\\nto suboptimal performance. As shown in S3, the results in\\nnoticeable sliding steps and unnatural movements.\\nS2. Details of VehExecutor\\nS2.1. Network Architecture and Parameters of\\nBicycle-model\\nAll networks in VehExecutor are implemented as MLPs.\\nThe policy network consists of layers with dimensions 256,\\n256, 128, 128, 64, and 64, while the value network has\\nlayers with dimensions 1024, 512, 256, and 128. During\\ntraining, the parameters of the bicycle model (L, W, lf , lr)\\nare set to two configurations: (2.7, 1.8, 0.9, 0.9) and (6.1,\\n2.5, 2.3, 2.0), mixed to accommodate vehicles of varying\\nsizes. These parameters can be adjusted as needed based on\\nspecific requirements, with the two configurations provided\\nhere serving as examples.\\nS2.2. Obstacle state'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 14}, page_content='sizes. These parameters can be adjusted as needed based on\\nspecific requirements, with the two configurations provided\\nhere serving as examples.\\nS2.2. Obstacle state\\nThe states of obstacles are composed of their orientation\\nand distance relative to the vehicle’s own coordinate system\\n(considering the radius of the obstacles). The state vector\\nis initialized with a maximum number of observable obsta-\\ncles. The vector is then populated with the specific identi-\\nfiers of the actual obstacles, and any remaining entries are\\nmasked. Obstacles that are too far away are directly ex-\\ncluded, meaning their states are not considered, and they\\ndo not contribute to the reward calculation. The distance\\nthreshold for exclusion is set to 10 in the experiment.\\nS2.3. Supplymentary Experiments\\nRobustness. We also provide the results using LQR [38]\\nin S1. It can be observed that LQR is capable of vehicle dy-\\nnamics generation from a planned trajectory to some extent.\\nHowever, the planned trajectory lacks relevant constraints,\\nwhich may lead to unreasonable turns or abrupt changes,\\ncausing LQR to often produce less than ideal results. Fur-\\nthermore, we tested the robustness of vehicle dynamics gen-\\neration by adding noise with a mean of 0 and variance of σ\\nto the planned trajectory. As shown in S2, the results show\\nMethods/Speed 0 5 10 20\\nLQR [38] 0.074/0.058 0.086/0.070 0.092/0.079 0.125/0.0108\\nTable S1. Position/velocity error of LQR.\\nσ PP [10] Xu et al. [77] LQR [38] ours'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 14}, page_content='Methods/Speed 0 5 10 20\\nLQR [38] 0.074/0.058 0.086/0.070 0.092/0.079 0.125/0.0108\\nTable S1. Position/velocity error of LQR.\\nσ PP [10] Xu et al. [77] LQR [38] ours\\n0.00 0.162/0.142 0.095/0.077 0.092/0.079 0.077/0.054\\n0.01 0.168/0.147 0.098/0.080 0.322/0.289 0.079/0.058\\n0.03 0.169/0.151 0.098/0.079 0.568/0.479 0.082/0.060\\nTable S2. Vehicle dynamics generation under Gaussian noise. σ\\nindicates the variance of noise.\\nthat LQR is highly sensitive to noise, often producing sig-\\nnificantly worse outcomes under its influence, while other\\nmethods are relatively less affected by the noise.\\nVisualization for effectiveness. In the supplementary\\nvideo, we provide results comparing the planned trajecto-\\nries without using VehExecutor and with VehExecutor. It\\nis evident that, without the involvement of VehExecutor,\\nthe dynamics generated by simply calculating heading be-\\ntween consecutive frames of the directly planned trajectory\\nare highly unnatural, exhibiting noticeable tail swings and\\nabrupt changes. In contrast, the results using VehExecutor\\nare much more realistic and natural. This demonstrates the\\nnecessity of VehExecutor, as trajectories without physical\\nconstraints are highly unnatural and impractical.\\nS3. Details of High-level Planning\\nS3.1. LLM-agent details\\nWe provide the relevant sample prompts for the LLM agent\\nin S7 and S8. All outputs from the LLM are in JSON for-\\nmat, and corresponding follow-up functions are used to con-'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 14}, page_content='S3.1. LLM-agent details\\nWe provide the relevant sample prompts for the LLM agent\\nin S7 and S8. All outputs from the LLM are in JSON for-\\nmat, and corresponding follow-up functions are used to con-\\nvert the JSON outputs into the required data structures.\\nSimilar to the rendering process, we also use the Waymo\\nOpen Dataset [60] as the planning dataset in all experi-\\nments, and the final results are presented based on this\\ndataset.\\nS3.2. Different LLMs\\nWe further validated the impact of different LLMs on the re-\\nsults in S3. We conducted experiments using GPT-3.5 [49]\\nand Llama-3 [16] 70B (smaller models struggle to accu-\\nrately execute the instructions). All other experimental set-\\ntings remained consistent with those in the main text. It is\\nevident that while other LLMs can handle the task to some\\nextent, GPT-4 [1] demonstrates the most accurate under-\\nstanding and decomposition of the instructions.\\nS3.3. Collision handling\\nIn the high-level planning process, we also designed a col-\\nlision handling mechanism to avoid unintended collisions.\\n3'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 15}, page_content='Methods Language command category Within roadsingle interaction compound\\nOurs-Llama3 0.885 0.742 0.812 0.920\\nOurs-GPT3.5 0.854 0.738 0.834 0.915\\nOurs-GPT4 0.952 0.883 0.896 0.935\\nTable S3. High-level planning evaluation for different LLMs.\\nChatSim [70] LCTGen [63] Ours\\nCollision rate 0.149 0.092 0.067\\nTable S4. Collision rate of high-level planning\\nSpecifically, during the trajectory generation, collision de-\\ntection is performed, and when a collision is detected, a ve-\\nlocity adjustment function is applied to modify the speed of\\none of the agents. This velocity adjustment function uses\\na nonlinear mapping to combine the original planned result\\nwith an interpolated trajectory, leading to a planning result\\nwith different speeds. We evaluated the probability of col-\\nlisions across 50 generated samples, which is calculated by\\ndividing the number of vehicles that experienced a collision\\nby the total number of vehicles. As shown in S4, the col-\\nlision rate for all methods remains low, and ChatDyn also\\nachieves a low collision rate while incorporating collision\\nhandling.\\n4'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 16}, page_content='AMP PPO PULSE Ours\\nKey frame 1 \\nKey frame 2 \\nKey frame 3 \\nPushing\\nFigure S4. Comparison of pushing.\\nAMP PPO PULSE Ours\\nKey frame 1 \\nKey frame 2 \\nKey frame 3 \\nPatting\\nFigure S5. Comparison of patting.\\n5'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 17}, page_content='AMP PPO PULSE Ours\\nKey frame 1 \\nKey frame 2 \\nKey frame 3 \\nWalking with arm around another’s shoulder\\nFigure S6. Comparison of walking with arm around another one’s shoulder.\\n6'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 18}, page_content='Oracle agent prompt.\\nI have a requirement for analyzing a scenario. I will provide you with a requirement, and I need your help to break it\\ndown into four pieces of information: (1) identify all the agents, (2) initialize each agent’s state, (3) formulate a text\\nfor each agent. You should provide the information in JSON format. Note that your output should only include the\\nJSON format of the information, not the analysis process.\\n(1) Identify all the agents: This means you need to extract all the agents from the scenario based on the input text.\\nThe text may be quite lengthy, so you can locate the agents by identifying the nouns, which may assist you in this\\ntask. The agents must be the objects involved in autonomous driving. The format should be as follows: agent list =\\n{’0’: ’agent name 0’, ’1’: ’agent name 1’, ...}. The keys (e.g., ’0’, ’1’, etc.) represent unique agent IDs, which you\\nshould assign starting from ’0’. The agents should only include vehicles like cars, pedestrians, trucks, and buses, and\\nshould not include static objects like trees or buildings. Ensure that each agent is given a distinct name. For example,\\nin the sentence ’car a wants to overtake car b’, the nouns are ’car a’ and ’car b’. Both are objects in autonomous\\ndriving scenarios. Therefore, the agent list should be formatted as follows: agent list = {’0’: ’car a’, ’1’: ’car b’}.\\nPedestrians can be represented with identifiers like ‘ped 0‘ and may share numbering with entities such as ‘car 0‘.'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 18}, page_content='Pedestrians can be represented with identifiers like ‘ped 0‘ and may share numbering with entities such as ‘car 0‘.\\n(2) Initialize each agent’s state. In this task, you need to determine four aspects for each agent: 1. Agent type, 2.\\nMovement, 3. Speed. You can use the results from task (1) to complete this task. Provide the initial state of each agent\\nin a list, formatted in JSON. Most time the speed is bigger than 0. For example, the initial states should be defined\\nas follows: init-states = [’agent id’: ’0’, ’agent type’: ’car’, ’movement’: ’overtaking’, ’speed’: 60, ’agent id’: ’1’,\\n’agent type’: ’car’, ’movement’: ’straight’, ’speed’: 30]. If one car intends to overtake another, it should ideally be\\nat least twice as fast as the car it is overtaking. The initial state must include the agent type, movement,and speed.\\nAgent type should in [pedestrian,vehicle], action should in [’static’, ’straight’,’pull over’, ’turn over’,’overtake’,’turn\\nleft’,’turn right’,’straight left’,’straight right’] if it’s a vehicle, and in [’static’,’crossing’,’straight’] if it is a pedestrian.\\n(3) Formulate a text for each agent. As an omniscient observer, you should instruct each agent on their actions through\\na text. The text should contain two pieces of information: 1. The agent type, 2. The agent’s intention. You need to\\nprovide this in a JSON format. guide texts = ’0’: ’text1’, ’1’: ’text2’, ’2’: ’text3’, ... where the key is the agent’s ID'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 18}, page_content='provide this in a JSON format. guide texts = ’0’: ’text1’, ’1’: ’text2’, ’2’: ’text3’, ... where the key is the agent’s ID\\nand the value is the text. The agent’s name should be consistent with those in the agent list.\\nYour answer should be in a JSON format,and must include the three information:agent list,init-state,guide texts.And\\ninit-state must include the agent type,movement and speed.\\nFigure S7. Oracle agent prompt.\\n7'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 19}, page_content='Actor agent prompt.\\nNow,you are an agent in the autonomous driving scenario.I will give you a text,and agent list, describing who you\\nare and what you need to do. Note that your output should not include your analysis process, only the JSON format\\nof the information you provide.\\nI need you to analyze the text and give me the four information of the ego agent to describe what type of lane the\\nagent should be in: (1)depend (2) speed change (3) keypoints list (4) behavior. Note that you only need to give the\\ninformation of the ego agent,not the other agents.\\n(1) depend. You need to determine the depend of the agent according to the agent’s intention. And give it\\nlike [depend agent id,depend type].You can get the depend agent id from the agent list. Depend type should in\\n[’end’,’start’,’trajectory’,’None’].’end’ represents the ego agent’s end point is the depend agent’s end point, ’start’\\nrepresents the ego agent’s start point is the depend agent’s start point, ’trajectory’ represents the ego agent’s whole\\ntrajectory is the depend agent’s start point. If it has no depend, you should give it [-1,’None’].\\n(2) speed change. You need to determine the speed change of the agent according to the agent’s intention. If you want\\nto speed up, the speed change should be 1. If you want to slow down, the speed change should be -1. If you want to\\nkeep the speed, the speed change should be 0. For example, If there is someone nearby, you should slow down.'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 19}, page_content='keep the speed, the speed change should be 0. For example, If there is someone nearby, you should slow down.\\n(3) keypoints list. You need to determine the number of keypoints required for your current behavior, the confirma-\\ntion method for each keypoint, and their respective parameters, then return a list containing this information. Each\\nkeypoint can be confirmed in one of three ways: (1) Map-based (type ‘0‘), with parameters: lane position (‘left‘,\\n‘right‘, or ‘front‘), lane type (‘centerline‘ or ‘boundary‘), and driving direction (‘turn left‘, ‘turn right‘, or ‘straight‘).\\n(2) Lane-relationship-based (type ‘1‘), with the parameter being the relationship to the previous keypoint (‘opposite\\ndirection adjacent‘, ‘same direction adjacent‘, ‘adjacent straight‘, ‘adjacent left turn‘, ‘adjacent right turn‘, ‘different\\ntype adjacent‘, or ‘opposite boundary‘). (3) Agent-based (type ‘2‘), with parameters specifying required information\\ntype (‘point‘ or ‘trajectory‘). The result should be a sequential list of dictionaries where the key is the type (‘0‘, ‘1‘,\\nor ‘2‘) and the value is the corresponding parameters. For example, a left-turning car might return ‘[’0’: ’position’:\\n’front’, ’lane type’: ’centerline’, ’direction’: ’turn left’, ’1’: ’relationship’: ’adjacent straight’]‘, while a car in the\\nleft lane picking up ‘ped a‘ might return ‘[’0’: ’position’: ’left’, ’lane type’: ’centerline’, ’direction’: ’straight’,'),\n",
       " Document(metadata={'source': './test data/testpaper.pdf', 'page': 19}, page_content='left lane picking up ‘ped a‘ might return ‘[’0’: ’position’: ’left’, ’lane type’: ’centerline’, ’direction’: ’straight’,\\n’2’: ’info type’: ’point’]‘. Ensure compliance with parameters, common sense, and traffic regulations, with the first\\nkeypoint typically using type ‘0‘.\\n(4) behavior. You need to determine your behavior. If you are a vehicle, the behavior is ”None.” If you are a\\npedestrian, the behavior corresponds to the description provided. There are two scenarios: if your behavior is a\\nspecific action such as calling or waving, simply return the text of that action; if the behavior involves interactive\\nactions such as pushing, patting, or walking with an arm around another person, you must return the exact predefined\\ndescriptions for these three types of interactions without modification. For example, if you are calling, your behavior\\nis ”calling phone.”; if you push ped 1, your behavior is ”pushing.”\\nYour answer should be in a JSON format, and must include depend, speed change, keypoints list and behavior.\\nFigure S8. Actor agent prompt.\\n8')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using RecursiveCharacterTextSplitter from langchain to split the text into chunks\n",
    "\"\"\"\n",
    "Parameters:\n",
    "chunk_size is the maximum number of characters in each chunk, \n",
    "chunk_overlap is the number of characters to overlap between chunks so each chunk has some context from the previous chunk,\n",
    "length_function is how we want to measure the length of each chunk i.e how we want to count the chunks,\n",
    "separators is used so that we dont split in the middle of a word, or sentence etc we say, sperate on either page break or new line or space\n",
    "\"\"\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200, length_function=len, separators=['\\n\\n', '\\n', \" \"]) \n",
    "# running the text splitter on test paper and storing the chunks\n",
    "pdf_chunks = text_splitter.split_documents(pdf_pages) # retuns list of chunks \n",
    "pdf_chunks # printing the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to repersent the chunks numarically this is where we will use text embeddings\n",
    "\n",
    "Text embeddings are a way of repersenting words or documents as numarical vectors that capture there meaning. this was text can be converted to a format that compueters can understand and work with. These embedding vectors are lists of numbers where each number a vector in space. These vector values dont have any real meaning on there own, but relationships between vectors dose have meaning and is important. EX: simmilar words will have simmilar vectors meaning there vectors will we closer together in space and vise versa. How do we know if there far or close? The distance between these vectors can be calculated using cosine similarity or euclidean distance. we dont need to calulate this ourself as there libraries to do that, but linear algebra is important to understand how this works. There are also many types of embedding models ranging from simple to complex. A better model can help capture the meaning of text better so having good embedding for our chunks is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using open ai's embeddings library to embed the chunks\n",
    "def get_embeddings():\n",
    "    # load the embeddings model \n",
    "    embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=OPENAPI_API_KEY)\n",
    "    return embeddings_model # returning the embeddings model\n",
    "\n",
    "embedding_model = get_embeddings()\n",
    "test_vector = embedding_model.embed_query(\"test\") # embedding the query test this will return us a large vector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Distance Between Two Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.18510032813291866}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using langchains evaluator to evaluate the embeddings \n",
    "\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\", embeddings=embedding_model) # loading the evaluator with our evaluator type and embeddings model\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"Man\", reference=\"Woman\") # evaluating the embeddings of man and woman\n",
    "evaluator.evaluate_strings(prediction=\"Man\", reference=\"Queen\") # evaluating the embeddings of man and Queen\n",
    "# here in the frist result the prediction and reference are more similar than the second result\n",
    "# both evaluators return a score between 0 and 1 repersenting the similarity of the embeddings, the first evaluator is a higher score than the second evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have alot of vectors because we have alot of words. We need a way to manage and query these vectors. So we use a database, a vector database. in our case we use Chroma DB\n",
    "\n",
    "A Vector database is like a library we have our stuff organized and we can find it by looking up the name, insted of books we store chunks of information repersented as vectors. Chroma is a open source fast and scalable vector database, But there are others. How dose a vector database work? When we make a query like asking a question, how dose this book end? the database lloks at the question, creates a vector embedding for it, scans through all the vector embeddings in the datbase to find the ones that are most simmilar to the vectors of the question. Then it retuns the coresponding chunks that are most simmilar to the question. These relevent chunks can be put togther and fed into a llm like gpt4o to generate a good answer to our answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using chroma ds to made a vector store the vectors of the chunks\n",
    "# the function allows us to make a whole new vector store, NOTE: if we make more than one embedding for a file it will be sotred as two chunks (AVOID THIS)\n",
    "def create_vector_store(pdf_chunks, embedding_model, store_name):\n",
    "    # passing our pdf and embeddings model to the database, we store the database in a local folder called vector store so we can load it later on\n",
    "    vectorstore = Chroma.from_documents(documents=pdf_chunks, embedding=embedding_model, persist_directory=store_name) \n",
    "    vectorstore.persist() # persisting the vector store to make the directory (for making sure the filder in made)\n",
    "    return vectorstore # returning the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY DATABASE FOR RELEVANT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/23/rj3hwzds32x8m0hj4z25n9kh0000gn/T/ipykernel_42947/2563829399.py:6: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist() # persisting the vector store to make the directory (for making sure the filder in made)\n"
     ]
    }
   ],
   "source": [
    "# load the Vector database using the vectorstore function\n",
    "vectorstore = create_vector_store(pdf_chunks, embedding_model, \"vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 7, 'source': './test data/testpaper.pdf'}, page_content='Methods FID↓ Div.↑ l-FID↓ l-Div.↑ Empjpe↓ Ef↓ UP↑\\nPacer[56] 7.25 1.24 7.93 1.05 \\\\ 0.122 0.179\\nPacer+[69] 6.62 1.58 7.76 1.28 82.33 0.128 0.289\\nOurs 6.21 1.76 7.07 1.49 79.82 0.124 0.532\\nTable 2. Dynamics quality, diversity and accuracy evaluation for\\ntrajectory following and motion imitation tasks.\\nMethods Unified policy Interaction 1 Interaction 2 Interaction 3\\nPPO [57] × 0.971 0.934 0.914\\nAMP [51] × 0.234 0.179 0.108\\nPulse [45] × 0.975 0.942 0.925\\nOurs ✓ 0.982 0.977 0.971\\nTable 3. Interaction tasks success rate evaluation.\\n4.3. Component Results\\n4.3.1. LLM-agents planning\\nWe compare the planned trajectory generated by LLM\\nagents with existing traffic generation methods based on\\nlanguage, including LCTGen [63] and ChatSim [70]. Since\\nother methods do not support scenarios involving pedestri-\\nans, we conduct an evaluation under vehicle-only instruc-\\ntions. We define three types of instructions: single-vehicle\\ninstructions, interaction instructions, and composite instruc-\\ntions. Using 5 maps, we generate 26 samples per instruction\\ntype and invite 10 users to evaluate each generated result.\\nThey assess whether each result matches the language de-\\nscription, whether it is within-road, and their preference for\\neach sample. The proportion of positive responses for each\\ncriterion is summarized in Table 1. The results indicate that\\nChatDyn consistently produced more accurate planning re-\\nsults that matched descriptions and were preferred by users.'),\n",
       " Document(metadata={'page': 7, 'source': './test data/testpaper.pdf'}, page_content='criterion is summarized in Table 1. The results indicate that\\nChatDyn consistently produced more accurate planning re-\\nsults that matched descriptions and were preferred by users.\\nWe also provide visual examples in Figure 6, which also in-\\nvolves some planning results of ChatDyn with pedestrians.\\nLCTGen struggles with interaction controls and complex\\ninstructions, while ChatDyn accurately fulfill all require-\\nments, producing high-quality outputs that meet specifica-\\ntions, even in scenarios involving pedestrians.\\n4.3.2. Pedestrian executor\\nWe follow the evaluation from Pacer+ [69] to assess tra-\\njectory following and motion imitation tasks for PedExecu-\\ntor as shown in Table 2. The quality and diversity of the\\ngenerated results are measured by Frechet Inception Dis-\\ntance (FID) [27] and diversity metric (Div.) [69] on nor-\\nmal speed, while l-FID and l-Div. are the metrics on low\\nspeed; imitation accuracy by Mean Per-Joint Position Error\\n(Empjpe), and following accuracy by following error (Ef ).\\nAdditionally, user preference (UP) is assessed by inviting\\n15 users to evaluate 33 segments of dynamics. With the\\nsupport of hierarchical control and related training strategy,\\nPedExecutor generates higher quality dynamics and demon-\\nstrated highly competitive performance in both following\\nand imitation tasks.\\nWe also evaluate the process of performing interaction\\nTE HC M. AMP Success rate User preference\\n× ✓ ✓ 0.403 0.120\\n✓ ✓ × 0.186 0.069\\n✓ × ✓ 0.948 0.344\\n✓ ✓ ✓ 0.977 0.467'),\n",
       " Document(metadata={'page': 6, 'source': './test data/testpaper.pdf'}, page_content='ing, walking with arm around shoulder). These interactions\\namong different types of traffic participants contribute to a\\nmore diverse and realistic scene dynamics; (ii) the system\\nachieves precise and detailed control, with each scene de-\\nscription being complex, potentially including abstract se-\\nmantics. Through the design of multi-LLM-agents role-\\nplaying, complex instructions are accurately analyzed and\\nexecuted, and abstract semantic information is effectively\\nbroken down into executable commands for final genera-\\ntion; (iii) the generated results are highly realistic. The\\nphysics-based control policies for pedestrians and vehicles\\nproduce realistic, and intuitive dynamics, particularly high-\\nlighting physical feedback in human-human interaction.\\n7'),\n",
       " Document(metadata={'page': 11, 'source': './test data/testpaper.pdf'}, page_content='In Conference on Robot Learning , pages 144–177. PMLR,\\n2023. 1, 3\\n[83] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen,\\nSushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone.\\nGuided conditional diffusion for controllable traffic simula-\\ntion. In 2023 IEEE International Conference on Robotics\\nand Automation (ICRA), pages 3560–3566. IEEE, 2023. 3\\n[84] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert\\nLo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan\\nBisk, Daniel Fried, et al. Webarena: A realistic web en-\\nvironment for building autonomous agents. arXiv preprint\\narXiv:2307.13854, 2023. 3\\n[85] Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng\\nLiao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura,\\nWenping Wang, and Lingjie Liu. Emdm: Efficient motion\\ndiffusion model for fast and high-quality motion generation.\\nIn European Conference on Computer Vision, pages 18–38.\\nSpringer, 2025. 2\\n12')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creata a data retriever from the vector store\n",
    "# as retriever from langchain, search type is similarity it uses cosine distance, it will by default return the 4 most relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\") \n",
    "relevant_chunks = retriever.invoke(\"What is the test paper about\") # calling the retriever to get the relevant chunks from the vector store for our given question\n",
    "relevant_chunks # printing the relevant chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A PROMT FOR THE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promt template this is our gpt prompt start to tell gpt the context of what we are doing\n",
    "# we have 2 place holders {context} and {question} that will be given to gpt when we call it\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant that can answer questions about a PDF file.\n",
    "Use the following pieces of context to answer the question, if you don't know the answer, \n",
    "just say that you don't know, don't try to make up an answer DONT DO IT DONT DO IT !!!!!\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the context given above: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate all the relevent context into one string\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks]) \n",
    "\n",
    "# create the final prompt\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template) # creating the prompt using the chat prompt template library\n",
    "final_prompt = prompt.format(context=context_text, question=\"What is the test paper about\") # passing in the context and question to the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASSINNG THE PROMT TO THE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The test paper discusses the evaluation of various methods for trajectory following and motion imitation tasks, particularly focusing on the performance of a system called ChatDyn compared to existing traffic generation methods. It emphasizes the quality, diversity, and accuracy of planned trajectories generated by LLM agents. The paper includes metrics for assessing the performance of different methods, such as Frechet Inception Distance (FID), diversity metrics, and user preference evaluations. Additionally, it highlights the effectiveness of ChatDyn in producing accurate planning results, especially in scenarios involving pedestrians, and the use of hierarchical control and training strategies in its PedExecutor component. Overall, it addresses advancements in traffic simulation and agent-based interaction dynamics.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 1302, 'total_tokens': 1435, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-b795493b-edb9-4584-8cd4-dae2e31ba395-0', usage_metadata={'input_tokens': 1302, 'output_tokens': 133, 'total_tokens': 1435, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally we pass this promt into the actual llm (like giving gpt a message)\n",
    "llm.invoke(final_prompt) # calling the llm for the final prompt, we will get back a response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG CHAIN EQUVALENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs): # function to format the doc passed in\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# this rag chain first gets the relevent chunks from the vector store then we concatinate our relevant chunks into one string \n",
    "# and we pass the context and question into the prompt template and that promt is passed into the llm \n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "        )\n",
    "rag_chain.invoke(\"What's the title of this paper?\") # same output as above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
