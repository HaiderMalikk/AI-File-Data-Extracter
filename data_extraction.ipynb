{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:\n",
    "langchain for the ai in app integration and openai specific integration\n",
    "chromadb is the vector database for storing and querying data\n",
    "pypdf for parsing and reading pdfs in python\n",
    "pandas for data manipulation and analysis\n",
    "streamlit for the app UI\n",
    "dotenv for managing environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet langchain-community langchain-openai chromadb\n",
    "!pip3 install --upgrade --quiet pypdf pandas streamlit python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Api key is in env file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langchain modules for alot of things\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Other modules and packages that are needed\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st  \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # reading all vaiables from .env file (api key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAPI_API_KEY = os.environ.get('OPENAI_API_KEY') # getting api key from .env file and bringing it to our notebook as a var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from langchain_openai we get our llm and specify the model (4o is cheap and fast)\n",
    "    api key is optional here it will know our api key as we set the env var\"\"\"\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAPI_API_KEY, temperature=0.2, max_tokens=2048)\n",
    "llm.invoke(\"if active respond with active\") # calling the llm for a prompt this is just like typing a message into chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSING THE PDF FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"./test data/testpaper.pdf\") # loading the pdf file from our project directory\n",
    "pdf_pages = pdf_loader.load() # loading the pages of the pdf\n",
    "pdf_pages # printing all the pages of the pdf\n",
    "\n",
    "\"\"\" pdf_pages contains a list of document objects, each document object representing a page of the pdf\n",
    "    the metadata contains the source of the document and the page number etc etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem, right now the pdfpages contains the whole pdf as you might have through there is no way we will put in a multi page reserach paper into open ai's llm model, firstly there a token limit, secondly and more importantly we need to specify parts in the document to get good results i.e the llm dose not need every word in the pdf, hence we only want to feed the most relevent part into the llm promt. passing too much info/ irelevent info to the llm gives bad results.\n",
    "- Solution, split the pdf into smaller chunks like paragaphs/ sentences. as we slipt he document into smaller chunks each chunk will be more relevent and contain less data making our resulting prompt more accurate and more likely to get good results from the llm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RecursiveCharacterTextSplitter from langchain to split the text into chunks\n",
    "\"\"\"\n",
    "Parameters:\n",
    "chunk_size is the maximum number of characters in each chunk, \n",
    "chunk_overlap is the number of characters to overlap between chunks so each chunk has some context from the previous chunk,\n",
    "length_function is how we want to measure the length of each chunk i.e how we want to count the chunks,\n",
    "separators is used so that we dont split in the middle of a word, or sentence etc we say, sperate on either page break or new line or space\n",
    "\"\"\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200, length_function=len, separators=['\\n\\n', '\\n', \" \"]) \n",
    "# running the text splitter on test paper and storing the chunks\n",
    "pdf_chunks = text_splitter.split_documents(pdf_pages) # retuns list of chunks \n",
    "pdf_chunks # printing the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to repersent the chunks numarically this is where we will use text embeddings\n",
    "\n",
    "Text embeddings are a way of repersenting words or documents as numarical vectors that capture there meaning. this was text can be converted to a format that compueters can understand and work with. These embedding vectors are lists of numbers where each number a vector in space. These vector values dont have any real meaning on there own, but relationships between vectors dose have meaning and is important. EX: simmilar words will have simmilar vectors meaning there vectors will we closer together in space and vise versa. How do we know if there far or close? The distance between these vectors can be calculated using cosine similarity or euclidean distance. we dont need to calulate this ourself as there libraries to do that, but linear algebra is important to understand how this works. There are also many types of embedding models ranging from simple to complex. A better model can help capture the meaning of text better so having good embedding for our chunks is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using open ai's embeddings library to embed the chunks\n",
    "def get_embeddings():\n",
    "    # load the embeddings model \n",
    "    embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=OPENAPI_API_KEY)\n",
    "    return embeddings_model # returning the embeddings model\n",
    "\n",
    "embedding_model = get_embeddings()\n",
    "test_vector = embedding_model.embed_query(\"test\") # embedding the query test this will return us a large vector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Distance Between Two Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using langchains evaluator to evaluate the embeddings \n",
    "\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\", embeddings=embedding_model) # loading the evaluator with our evaluator type and embeddings model\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"Man\", reference=\"Woman\") # evaluating the embeddings of man and woman\n",
    "evaluator.evaluate_strings(prediction=\"Man\", reference=\"Queen\") # evaluating the embeddings of man and Queen\n",
    "# here in the frist result the prediction and reference are more similar than the second result\n",
    "# both evaluators return a score between 0 and 1 repersenting the similarity of the embeddings, the first evaluator is a higher score than the second evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have alot of vectors because we have alot of words. We need a way to manage and query these vectors. So we use a database, a vector database. in our case we use Chroma DB\n",
    "\n",
    "A Vector database is like a library we have our stuff organized and we can find it by looking up the name, insted of books we store chunks of information repersented as vectors. Chroma is a open source fast and scalable vector database, But there are others. How dose a vector database work? When we make a query like asking a question, how dose this book end? the database lloks at the question, creates a vector embedding for it, scans through all the vector embeddings in the datbase to find the ones that are most simmilar to the vectors of the question. Then it retuns the coresponding chunks that are most simmilar to the question. These relevent chunks can be put togther and fed into a llm like gpt4o to generate a good answer to our answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using chroma ds to made a vector store the vectors of the chunks\n",
    "# the function allows us to make a whole new vector store, NOTE: if we make more than one embedding for a file it will be sotred as two chunks (AVOID THIS)\n",
    "def create_vector_store(pdf_chunks, embedding_model, store_name):\n",
    "    # passing our pdf and embeddings model to the database, we store the database in a local folder called vector store so we can load it later on\n",
    "    vectorstore = Chroma.from_documents(documents=pdf_chunks, embedding=embedding_model, persist_directory=store_name) \n",
    "    vectorstore.persist() # persisting the vector store to make the directory (for making sure the filder in made)\n",
    "    return vectorstore # returning the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY DATABASE FOR RELEVANT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Vector database using the vectorstore function\n",
    "vectorstore = create_vector_store(pdf_chunks, embedding_model, \"vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creata a data retriever from the vector store\n",
    "# as retriever from langchain, search type is similarity it uses cosine distance, it will by default return the 4 most relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\") \n",
    "relevant_chunks = retriever.invoke(\"What is the test paper about\") # calling the retriever to get the relevant chunks from the vector store for our given question\n",
    "relevant_chunks # printing the relevant chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A PROMT FOR THE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promt template this is our gpt prompt start to tell gpt the context of what we are doing\n",
    "# we have 2 place holders {context} and {question} that will be given to gpt when we call it\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant that can answer questions about a PDF file.\n",
    "Use the following pieces of context to answer the question, if you don't know the answer, \n",
    "just say that you don't know, don't try to make up an answer DONT DO IT DONT DO IT !!!!!\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the context given above: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate all the relevent context into one string\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks]) \n",
    "\n",
    "# create the final prompt\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template) # creating the prompt using the chat prompt template library\n",
    "final_prompt = prompt.format(context=context_text, question=\"What is the Title, Summary and Publication date and author of the test paper\") # passing in the context and question to the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG CHAIN EQUVALENT OF STEPS BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs): # function to format the doc passed in\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# this rag chain first gets the relevent chunks from the vector store then we concatinate our relevant chunks into one string \n",
    "# and we pass the context and question into the prompt template and that promt is passed into the llm \n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "        )\n",
    "rag_chain.invoke(\"What is the Title, Summary and Publication date and author of the test paper\") # same output as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRUCTURING THE GENERATED RESPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pydantic a data validation library too specify the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this class to get sources and resoning for our answer \n",
    "class AnswerWithSources(BaseModel):\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on the sources\")\n",
    "    \n",
    "# this class defines the structure for the data here we pass the questions into the llm and get a asnwer for each question that folows the structure of the class above\n",
    "class ExtractedInfo(BaseModel):\n",
    "    pdf_title: AnswerWithSources # title of the pdf\n",
    "    pdf_summary: AnswerWithSources # summary of the pdf\n",
    "    pdf_publication_date_and_author: AnswerWithSources # publication date and author of the pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INVOKE THE LLM WITH THE STRUCTURED RESPONSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the LLM directly with structured output\n",
    "response = llm.with_structured_output(ExtractedInfo, strict=True).invoke(final_prompt)\n",
    "# Print the structured response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMING STRUCTURED RESPONSE INTO A TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas to create a table to organize our data\n",
    "structured_response = response\n",
    "df = pd.DataFrame([structured_response.dict()])\n",
    "\n",
    "# Transforming into a table with three rows onw for each field in the answer\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with rows\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
