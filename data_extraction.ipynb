{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:\n",
    "langchain for the ai in app integration and openai specific integration\n",
    "chromadb is the vector database for storing and querying data\n",
    "pypdf for parsing and reading pdfs in python\n",
    "pandas for data manipulation and analysis\n",
    "streamlit for the app UI\n",
    "dotenv for managing environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet langchain-community langchain-openai chromadb\n",
    "!pip3 install --upgrade --quiet pypdf pandas streamlit python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Api key is in env file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langchain modules for alot of things\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Other modules and packages that are needed\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st  \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # reading all vaiables from .env file (api key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAPI_API_KEY = os.environ.get('OPENAI_API_KEY') # getting api key from .env file and bringing it to our notebook as a var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from langchain_openai we get our llm and specify the model (4o is cheap and fast)\n",
    "    api key is optional here it will know our api key as we set the env var\"\"\"\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAPI_API_KEY)\n",
    "llm.invoke(\"if active respond with active\") # calling the llm for a prompt this is just like typing a message into chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSING THE PDF FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"./test data/testpaper.pdf\") # loading the pdf file from our project directory\n",
    "pdf_pages = pdf_loader.load() # loading the pages of the pdf\n",
    "pdf_pages # printing all the pages of the pdf\n",
    "\n",
    "\"\"\" pdf_pages contains a list of document objects, each document object representing a page of the pdf\n",
    "    the metadata contains the source of the document and the page number etc etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem, right now the pdfpages contains the whole pdf as you might have through there is no way we will put in a multi page reserach paper into open ai's llm model, firstly there a token limit, secondly and more importantly we need to specify parts in the document to get good results i.e the llm dose not need every word in the pdf, hence we only want to feed the most relevent part into the llm promt. passing too much info/ irelevent info to the llm gives bad results.\n",
    "- Solution, split the pdf into smaller chunks like paragaphs/ sentences. as we slipt he document into smaller chunks each chunk will be more relevent and contain less data making our resulting prompt more accurate and more likely to get good results from the llm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RecursiveCharacterTextSplitter from langchain to split the text into chunks\n",
    "\"\"\"\n",
    "Parameters:\n",
    "chunk_size is the maximum number of characters in each chunk, \n",
    "chunk_overlap is the number of characters to overlap between chunks so each chunk has some context from the previous chunk,\n",
    "length_function is how we want to measure the length of each chunk i.e how we want to count the chunks,\n",
    "separators is used so that we dont split in the middle of a word, or sentence etc we say, sperate on either page break or new line or space\n",
    "\"\"\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200, length_function=len, separators=['\\n\\n', '\\n', \" \"]) \n",
    "# running the text splitter on test paper and storing the chunks\n",
    "pdf_chunks = text_splitter.split_documents(pdf_pages) # retuns list of chunks "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
